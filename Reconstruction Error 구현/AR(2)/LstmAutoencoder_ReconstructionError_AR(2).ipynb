{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorchtools import EarlyStopping\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy & Python Version 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 버전: 1.24.3\n",
      "Python 버전: 3.11.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import platform\n",
    "\n",
    "# NumPy 버전 확인\n",
    "numpy_version = np.__version__\n",
    "\n",
    "# Python 버전 확인\n",
    "python_version = platform.python_version()\n",
    "\n",
    "print(f\"NumPy 버전: {numpy_version}\")\n",
    "print(f\"Python 버전: {python_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA 사용 및 EarlyStopping 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "early_stopping = EarlyStopping(patience = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel():\n",
    "    torch.save(model.state_dict(), 'model_lstm_autoencoder_AR(2).pt') # 모델의 학습된 매개변수 파일에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "length = 12            # 윈도우 사이즈 (생성할 시계열 데이터의 길이)\n",
    "trainrun = 200       # 생성할 학습 데이터 시퀀스의 수 \n",
    "validrun = 100         # 생성할 검증 데이터 시퀀스의 수 \n",
    "\n",
    "# 시계열 데이터 생성을 위한 매개변수\n",
    "# 자기상관계수 (phi1, phi2별로 생성)\n",
    "phi1 = np.array([0, 0.25, 0.4, 0.6, 0.8])\n",
    "phi2 = np.array([0.1, 0.2, 0.3, 0.2, 0.1])\n",
    "\n",
    "# 변화율 크기\n",
    "psi1 = 0\n",
    "\n",
    "# 공정의 수준 변화율 (delta)\n",
    "de1 = 0\n",
    "\n",
    "# 공정의 분산 변화율 (gamma)\n",
    "ga = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시계열 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# AR(2) 시계열 데이터 생성 함수\n",
    "def ar2(phi1, phi2, delta, gamma, psi, length, run):\n",
    "    # 초기 설정\n",
    "    y = np.zeros(shape=(run, length))                                                         # 생성될 시계열 데이터를 저장할 빈 배열을 초기화. 배열의 크기는 (생성할 데이터 시퀀스의 수, 각 시퀀스의 길이) \n",
    "    sigma = math.sqrt((1 - phi2) / ((1 + phi2) * (1 - phi2 - phi1) * (1 - phi2 + phi1)))     # AR(2)모델의 표준 편차\n",
    "    \n",
    "    # 데이터 시퀀스 생성\n",
    "    for j in range(0, run):                                     # 각 run 마다 랜덤 노이즈(e)를 정규분포에서 추출하여 시계열의 기본 노이즈 생성 (과적합 방지 차원)\n",
    "        e = np.random.normal(loc=0, scale=1, size=length)       \n",
    "        x = np.zeros(length)\n",
    "        \n",
    "        x[0] = e[0]                                             # x 배열 초기화하고, 첫 번째 시점의 값은 첫 번째 노이즈 값으로 설정 (시계열의 시작점에서 발생할 수 있는 임의성 반영 및 자기상관 구조 구현)\n",
    "\n",
    "        # psi 시점 이전의 데이터 생성\n",
    "        for i in range(2, psi):                                 # psi 시점 이전까지는 관리상태 데이터\n",
    "            x[i] = phi1 * x[i - 1] + phi2 * x[i - 2] + e[i]     # AR(2)모델의 관리상태\n",
    "            \n",
    "        # psi 시점 이후의 데이터 생성 및 변동성 적용\n",
    "        for i in range(psi,len(x)):                             # AR(2)모델의 이상상태\n",
    "            e[i] = gamma * e[i]\n",
    "            x[i] = phi1 * x[i - 1] + phi2 * x[i - 2] + e[i] \n",
    "        for i in range(psi,len(x)):                             # delta(변동성 크기 조절하는 매개변수)를 통한 추가 변동성 적용\n",
    "            x[i] = x[i] + delta*sigma\n",
    "        \n",
    "        # 최종 데이터 반환 (각 run에 대해 생성된 시계열 데이터를 저장)  \n",
    "        y[j] = x\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "# 다양한 매개변수 조합에 대한 시계열 데이터 세트 생성\n",
    "def totaldat(run,length):\n",
    "    # 빈 데이터 배열 초기화\n",
    "    y = np.zeros(shape=(len(phi1), run, length))\n",
    "    # 매개변수 조합별 데이터 생성\n",
    "    for i, (phi_1, phi_2) in enumerate(zip(phi1, phi2)):\n",
    "        y[i]= ar2(phi_1, phi_2, de1, ga, psi1, length, run)\n",
    "\n",
    "    return y.reshape(run * len(phi1), length)                 # 생성된 데이터를 적절한 형태로 재배열\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "train_x = totaldat(trainrun,length)                             # 훈련용 시계열 데이터 생성\n",
    "train_x = train_x.reshape(trainrun*len(phi1),length)            \n",
    "\n",
    "# 검증용 시계열 데이터\n",
    "# 데이터 생성 및 변형\n",
    "valid_x = totaldat(run = validrun, length = length)\n",
    "valid_x = valid_x.reshape(validrun*len(phi1),length)\n",
    "\n",
    "# PyTorch 텐서로 변환 및 장치 할당\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_x = torch.FloatTensor(train_x).to(device)\n",
    "valid_x = torch.FloatTensor(valid_x).to(device)\n",
    "\n",
    "# DataLoader 설정\n",
    "trainloader = DataLoader(train_x, shuffle=True)                  # 데이터셋에서 미니배치 자동으로 생성 후 모델 학습 및 평가 시 배치 처리를 용이하게 함 (훈련에서는 데이터를 섞어 학습 과정에서의 일반화 능력 향상)\n",
    "validloader = DataLoader(valid_x, shuffle=False)                # 학습 및 검증에서는 데이터 순서 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "for data in trainloader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Autoencoder 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# LSTM Encoder 클래스 정의\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.hidden_to_latent(hidden[-1])\n",
    "\n",
    "# LSTM Decoder 클래스 정의\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, seq_length):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.outputs_to_data = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.latent_to_hidden(z).unsqueeze(0)\n",
    "        repeated_z = z.repeat(self.seq_length, 1, 1).transpose(0, 1)\n",
    "        lstm_out, _ = self.lstm(repeated_z)\n",
    "        return self.outputs_to_data(lstm_out)\n",
    "\n",
    "# 전체 LSTM 오토인코더 모델\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, hidden_dim=128, latent_dim=1):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# 모델 인스턴스 생성 및 설정\n",
    "model = LSTMAutoencoder(input_dim=length, seq_length=length, hidden_dim=128, latent_dim=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)  # You can adjust the initial lr based on your model and training needs\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.0194, Validation Loss: 1.0538\n",
      "Validation loss decreased (1.055042 --> 1.053813).  Saving model ...\n",
      "Epoch 2, Train Loss: 1.0183, Validation Loss: 1.0527\n",
      "Validation loss decreased (1.053813 --> 1.052656).  Saving model ...\n",
      "Epoch 3, Train Loss: 1.0171, Validation Loss: 1.0515\n",
      "Validation loss decreased (1.052656 --> 1.051549).  Saving model ...\n",
      "Epoch 4, Train Loss: 1.0160, Validation Loss: 1.0504\n",
      "Validation loss decreased (1.051549 --> 1.050397).  Saving model ...\n",
      "Epoch 5, Train Loss: 1.0150, Validation Loss: 1.0493\n",
      "Validation loss decreased (1.050397 --> 1.049318).  Saving model ...\n",
      "Epoch 6, Train Loss: 1.0140, Validation Loss: 1.0483\n",
      "Validation loss decreased (1.049318 --> 1.048257).  Saving model ...\n",
      "Epoch 7, Train Loss: 1.0129, Validation Loss: 1.0473\n",
      "Validation loss decreased (1.048257 --> 1.047256).  Saving model ...\n",
      "Epoch 8, Train Loss: 1.0119, Validation Loss: 1.0463\n",
      "Validation loss decreased (1.047256 --> 1.046262).  Saving model ...\n",
      "Epoch 9, Train Loss: 1.0110, Validation Loss: 1.0453\n",
      "Validation loss decreased (1.046262 --> 1.045294).  Saving model ...\n",
      "Epoch 10, Train Loss: 1.0100, Validation Loss: 1.0443\n",
      "Validation loss decreased (1.045294 --> 1.044305).  Saving model ...\n",
      "Epoch 11, Train Loss: 1.0091, Validation Loss: 1.0434\n",
      "Validation loss decreased (1.044305 --> 1.043353).  Saving model ...\n",
      "Epoch 12, Train Loss: 1.0083, Validation Loss: 1.0424\n",
      "Validation loss decreased (1.043353 --> 1.042399).  Saving model ...\n",
      "Epoch 13, Train Loss: 1.0074, Validation Loss: 1.0415\n",
      "Validation loss decreased (1.042399 --> 1.041546).  Saving model ...\n",
      "Epoch 14, Train Loss: 1.0065, Validation Loss: 1.0407\n",
      "Validation loss decreased (1.041546 --> 1.040671).  Saving model ...\n",
      "Epoch 15, Train Loss: 1.0057, Validation Loss: 1.0398\n",
      "Validation loss decreased (1.040671 --> 1.039777).  Saving model ...\n",
      "Epoch 16, Train Loss: 1.0049, Validation Loss: 1.0390\n",
      "Validation loss decreased (1.039777 --> 1.038952).  Saving model ...\n",
      "Epoch 17, Train Loss: 1.0041, Validation Loss: 1.0382\n",
      "Validation loss decreased (1.038952 --> 1.038240).  Saving model ...\n",
      "Epoch 18, Train Loss: 1.0033, Validation Loss: 1.0374\n",
      "Validation loss decreased (1.038240 --> 1.037365).  Saving model ...\n",
      "Epoch 19, Train Loss: 1.0025, Validation Loss: 1.0366\n",
      "Validation loss decreased (1.037365 --> 1.036558).  Saving model ...\n",
      "Epoch 20, Train Loss: 1.0018, Validation Loss: 1.0359\n",
      "Validation loss decreased (1.036558 --> 1.035869).  Saving model ...\n",
      "Epoch 21, Train Loss: 1.0010, Validation Loss: 1.0351\n",
      "Validation loss decreased (1.035869 --> 1.035128).  Saving model ...\n",
      "Epoch 22, Train Loss: 1.0003, Validation Loss: 1.0344\n",
      "Validation loss decreased (1.035128 --> 1.034358).  Saving model ...\n",
      "Epoch 23, Train Loss: 0.9996, Validation Loss: 1.0336\n",
      "Validation loss decreased (1.034358 --> 1.033583).  Saving model ...\n",
      "Epoch 24, Train Loss: 0.9989, Validation Loss: 1.0329\n",
      "Validation loss decreased (1.033583 --> 1.032928).  Saving model ...\n",
      "Epoch 25, Train Loss: 0.9982, Validation Loss: 1.0323\n",
      "Validation loss decreased (1.032928 --> 1.032270).  Saving model ...\n",
      "Epoch 26, Train Loss: 0.9975, Validation Loss: 1.0316\n",
      "Validation loss decreased (1.032270 --> 1.031565).  Saving model ...\n",
      "Epoch 27, Train Loss: 0.9969, Validation Loss: 1.0309\n",
      "Validation loss decreased (1.031565 --> 1.030932).  Saving model ...\n",
      "Epoch 28, Train Loss: 0.9962, Validation Loss: 1.0303\n",
      "Validation loss decreased (1.030932 --> 1.030329).  Saving model ...\n",
      "Epoch 29, Train Loss: 0.9956, Validation Loss: 1.0297\n",
      "Validation loss decreased (1.030329 --> 1.029689).  Saving model ...\n",
      "Epoch 30, Train Loss: 0.9950, Validation Loss: 1.0291\n",
      "Validation loss decreased (1.029689 --> 1.029075).  Saving model ...\n",
      "Epoch 31, Train Loss: 0.9943, Validation Loss: 1.0284\n",
      "Validation loss decreased (1.029075 --> 1.028439).  Saving model ...\n",
      "Epoch 32, Train Loss: 0.9937, Validation Loss: 1.0278\n",
      "Validation loss decreased (1.028439 --> 1.027842).  Saving model ...\n",
      "Epoch 33, Train Loss: 0.9931, Validation Loss: 1.0273\n",
      "Validation loss decreased (1.027842 --> 1.027277).  Saving model ...\n",
      "Epoch 34, Train Loss: 0.9926, Validation Loss: 1.0267\n",
      "Validation loss decreased (1.027277 --> 1.026687).  Saving model ...\n",
      "Epoch 35, Train Loss: 0.9920, Validation Loss: 1.0261\n",
      "Validation loss decreased (1.026687 --> 1.026092).  Saving model ...\n",
      "Epoch 36, Train Loss: 0.9914, Validation Loss: 1.0256\n",
      "Validation loss decreased (1.026092 --> 1.025594).  Saving model ...\n",
      "Epoch 37, Train Loss: 0.9908, Validation Loss: 1.0250\n",
      "Validation loss decreased (1.025594 --> 1.025037).  Saving model ...\n",
      "Epoch 38, Train Loss: 0.9903, Validation Loss: 1.0245\n",
      "Validation loss decreased (1.025037 --> 1.024519).  Saving model ...\n",
      "Epoch 39, Train Loss: 0.9898, Validation Loss: 1.0240\n",
      "Validation loss decreased (1.024519 --> 1.023968).  Saving model ...\n",
      "Epoch 40, Train Loss: 0.9892, Validation Loss: 1.0234\n",
      "Validation loss decreased (1.023968 --> 1.023406).  Saving model ...\n",
      "Epoch 41, Train Loss: 0.9887, Validation Loss: 1.0230\n",
      "Validation loss decreased (1.023406 --> 1.022958).  Saving model ...\n",
      "Epoch 42, Train Loss: 0.9882, Validation Loss: 1.0224\n",
      "Validation loss decreased (1.022958 --> 1.022417).  Saving model ...\n",
      "Epoch 43, Train Loss: 0.9877, Validation Loss: 1.0219\n",
      "Validation loss decreased (1.022417 --> 1.021870).  Saving model ...\n",
      "Epoch 44, Train Loss: 0.9872, Validation Loss: 1.0214\n",
      "Validation loss decreased (1.021870 --> 1.021441).  Saving model ...\n",
      "Epoch 45, Train Loss: 0.9867, Validation Loss: 1.0210\n",
      "Validation loss decreased (1.021441 --> 1.021020).  Saving model ...\n",
      "Epoch 46, Train Loss: 0.9862, Validation Loss: 1.0206\n",
      "Validation loss decreased (1.021020 --> 1.020563).  Saving model ...\n",
      "Epoch 47, Train Loss: 0.9858, Validation Loss: 1.0201\n",
      "Validation loss decreased (1.020563 --> 1.020075).  Saving model ...\n",
      "Epoch 48, Train Loss: 0.9853, Validation Loss: 1.0197\n",
      "Validation loss decreased (1.020075 --> 1.019687).  Saving model ...\n",
      "Epoch 49, Train Loss: 0.9849, Validation Loss: 1.0192\n",
      "Validation loss decreased (1.019687 --> 1.019174).  Saving model ...\n",
      "Epoch 50, Train Loss: 0.9844, Validation Loss: 1.0188\n",
      "Validation loss decreased (1.019174 --> 1.018750).  Saving model ...\n",
      "Epoch 51, Train Loss: 0.9840, Validation Loss: 1.0183\n",
      "Validation loss decreased (1.018750 --> 1.018345).  Saving model ...\n",
      "Epoch 52, Train Loss: 0.9835, Validation Loss: 1.0179\n",
      "Validation loss decreased (1.018345 --> 1.017934).  Saving model ...\n",
      "Epoch 53, Train Loss: 0.9831, Validation Loss: 1.0175\n",
      "Validation loss decreased (1.017934 --> 1.017462).  Saving model ...\n",
      "Epoch 54, Train Loss: 0.9827, Validation Loss: 1.0171\n",
      "Validation loss decreased (1.017462 --> 1.017076).  Saving model ...\n",
      "Epoch 55, Train Loss: 0.9823, Validation Loss: 1.0167\n",
      "Validation loss decreased (1.017076 --> 1.016731).  Saving model ...\n",
      "Epoch 56, Train Loss: 0.9819, Validation Loss: 1.0163\n",
      "Validation loss decreased (1.016731 --> 1.016347).  Saving model ...\n",
      "Epoch 57, Train Loss: 0.9815, Validation Loss: 1.0160\n",
      "Validation loss decreased (1.016347 --> 1.016028).  Saving model ...\n",
      "Epoch 58, Train Loss: 0.9811, Validation Loss: 1.0156\n",
      "Validation loss decreased (1.016028 --> 1.015558).  Saving model ...\n",
      "Epoch 59, Train Loss: 0.9807, Validation Loss: 1.0152\n",
      "Validation loss decreased (1.015558 --> 1.015205).  Saving model ...\n",
      "Epoch 60, Train Loss: 0.9803, Validation Loss: 1.0148\n",
      "Validation loss decreased (1.015205 --> 1.014818).  Saving model ...\n",
      "Epoch 61, Train Loss: 0.9799, Validation Loss: 1.0144\n",
      "Validation loss decreased (1.014818 --> 1.014450).  Saving model ...\n",
      "Epoch 62, Train Loss: 0.9796, Validation Loss: 1.0141\n",
      "Validation loss decreased (1.014450 --> 1.014100).  Saving model ...\n",
      "Epoch 63, Train Loss: 0.9792, Validation Loss: 1.0137\n",
      "Validation loss decreased (1.014100 --> 1.013738).  Saving model ...\n",
      "Epoch 64, Train Loss: 0.9788, Validation Loss: 1.0134\n",
      "Validation loss decreased (1.013738 --> 1.013411).  Saving model ...\n",
      "Epoch 65, Train Loss: 0.9785, Validation Loss: 1.0131\n",
      "Validation loss decreased (1.013411 --> 1.013104).  Saving model ...\n",
      "Epoch 66, Train Loss: 0.9781, Validation Loss: 1.0128\n",
      "Validation loss decreased (1.013104 --> 1.012782).  Saving model ...\n",
      "Epoch 67, Train Loss: 0.9778, Validation Loss: 1.0123\n",
      "Validation loss decreased (1.012782 --> 1.012335).  Saving model ...\n",
      "Epoch 68, Train Loss: 0.9775, Validation Loss: 1.0121\n",
      "Validation loss decreased (1.012335 --> 1.012066).  Saving model ...\n",
      "Epoch 69, Train Loss: 0.9771, Validation Loss: 1.0117\n",
      "Validation loss decreased (1.012066 --> 1.011737).  Saving model ...\n",
      "Epoch 70, Train Loss: 0.9768, Validation Loss: 1.0114\n",
      "Validation loss decreased (1.011737 --> 1.011411).  Saving model ...\n",
      "Epoch 71, Train Loss: 0.9765, Validation Loss: 1.0112\n",
      "Validation loss decreased (1.011411 --> 1.011168).  Saving model ...\n",
      "Epoch 72, Train Loss: 0.9761, Validation Loss: 1.0108\n",
      "Validation loss decreased (1.011168 --> 1.010834).  Saving model ...\n",
      "Epoch 73, Train Loss: 0.9758, Validation Loss: 1.0105\n",
      "Validation loss decreased (1.010834 --> 1.010508).  Saving model ...\n",
      "Epoch 74, Train Loss: 0.9755, Validation Loss: 1.0102\n",
      "Validation loss decreased (1.010508 --> 1.010232).  Saving model ...\n",
      "Epoch 75, Train Loss: 0.9752, Validation Loss: 1.0099\n",
      "Validation loss decreased (1.010232 --> 1.009884).  Saving model ...\n",
      "Epoch 76, Train Loss: 0.9749, Validation Loss: 1.0097\n",
      "Validation loss decreased (1.009884 --> 1.009677).  Saving model ...\n",
      "Epoch 77, Train Loss: 0.9746, Validation Loss: 1.0093\n",
      "Validation loss decreased (1.009677 --> 1.009321).  Saving model ...\n",
      "Epoch 78, Train Loss: 0.9743, Validation Loss: 1.0090\n",
      "Validation loss decreased (1.009321 --> 1.009018).  Saving model ...\n",
      "Epoch 79, Train Loss: 0.9740, Validation Loss: 1.0088\n",
      "Validation loss decreased (1.009018 --> 1.008793).  Saving model ...\n",
      "Epoch 80, Train Loss: 0.9738, Validation Loss: 1.0085\n",
      "Validation loss decreased (1.008793 --> 1.008534).  Saving model ...\n",
      "Epoch 81, Train Loss: 0.9735, Validation Loss: 1.0082\n",
      "Validation loss decreased (1.008534 --> 1.008240).  Saving model ...\n",
      "Epoch 82, Train Loss: 0.9732, Validation Loss: 1.0080\n",
      "Validation loss decreased (1.008240 --> 1.007987).  Saving model ...\n",
      "Epoch 83, Train Loss: 0.9729, Validation Loss: 1.0077\n",
      "Validation loss decreased (1.007987 --> 1.007712).  Saving model ...\n",
      "Epoch 84, Train Loss: 0.9726, Validation Loss: 1.0074\n",
      "Validation loss decreased (1.007712 --> 1.007393).  Saving model ...\n",
      "Epoch 85, Train Loss: 0.9724, Validation Loss: 1.0072\n",
      "Validation loss decreased (1.007393 --> 1.007152).  Saving model ...\n",
      "Epoch 86, Train Loss: 0.9721, Validation Loss: 1.0069\n",
      "Validation loss decreased (1.007152 --> 1.006862).  Saving model ...\n",
      "Epoch 87, Train Loss: 0.9719, Validation Loss: 1.0066\n",
      "Validation loss decreased (1.006862 --> 1.006597).  Saving model ...\n",
      "Epoch 88, Train Loss: 0.9716, Validation Loss: 1.0064\n",
      "Validation loss decreased (1.006597 --> 1.006393).  Saving model ...\n",
      "Epoch 89, Train Loss: 0.9714, Validation Loss: 1.0061\n",
      "Validation loss decreased (1.006393 --> 1.006139).  Saving model ...\n",
      "Epoch 90, Train Loss: 0.9711, Validation Loss: 1.0059\n",
      "Validation loss decreased (1.006139 --> 1.005903).  Saving model ...\n",
      "Epoch 91, Train Loss: 0.9708, Validation Loss: 1.0056\n",
      "Validation loss decreased (1.005903 --> 1.005624).  Saving model ...\n",
      "Epoch 92, Train Loss: 0.9706, Validation Loss: 1.0054\n",
      "Validation loss decreased (1.005624 --> 1.005414).  Saving model ...\n",
      "Epoch 93, Train Loss: 0.9704, Validation Loss: 1.0051\n",
      "Validation loss decreased (1.005414 --> 1.005139).  Saving model ...\n",
      "Epoch 94, Train Loss: 0.9701, Validation Loss: 1.0050\n",
      "Validation loss decreased (1.005139 --> 1.004962).  Saving model ...\n",
      "Epoch 95, Train Loss: 0.9699, Validation Loss: 1.0047\n",
      "Validation loss decreased (1.004962 --> 1.004731).  Saving model ...\n",
      "Epoch 96, Train Loss: 0.9697, Validation Loss: 1.0046\n",
      "Validation loss decreased (1.004731 --> 1.004603).  Saving model ...\n",
      "Epoch 97, Train Loss: 0.9694, Validation Loss: 1.0042\n",
      "Validation loss decreased (1.004603 --> 1.004235).  Saving model ...\n",
      "Epoch 98, Train Loss: 0.9692, Validation Loss: 1.0041\n",
      "Validation loss decreased (1.004235 --> 1.004052).  Saving model ...\n",
      "Epoch 99, Train Loss: 0.9690, Validation Loss: 1.0039\n",
      "Validation loss decreased (1.004052 --> 1.003891).  Saving model ...\n",
      "Epoch 100, Train Loss: 0.9688, Validation Loss: 1.0037\n",
      "Validation loss decreased (1.003891 --> 1.003668).  Saving model ...\n",
      "Epoch 101, Train Loss: 0.9685, Validation Loss: 1.0034\n",
      "Validation loss decreased (1.003668 --> 1.003411).  Saving model ...\n",
      "Epoch 102, Train Loss: 0.9683, Validation Loss: 1.0032\n",
      "Validation loss decreased (1.003411 --> 1.003185).  Saving model ...\n",
      "Epoch 103, Train Loss: 0.9681, Validation Loss: 1.0030\n",
      "Validation loss decreased (1.003185 --> 1.003040).  Saving model ...\n",
      "Epoch 104, Train Loss: 0.9679, Validation Loss: 1.0028\n",
      "Validation loss decreased (1.003040 --> 1.002782).  Saving model ...\n",
      "Epoch 105, Train Loss: 0.9677, Validation Loss: 1.0027\n",
      "Validation loss decreased (1.002782 --> 1.002708).  Saving model ...\n",
      "Epoch 106, Train Loss: 0.9675, Validation Loss: 1.0024\n",
      "Validation loss decreased (1.002708 --> 1.002397).  Saving model ...\n",
      "Epoch 107, Train Loss: 0.9673, Validation Loss: 1.0022\n",
      "Validation loss decreased (1.002397 --> 1.002165).  Saving model ...\n",
      "Epoch 108, Train Loss: 0.9671, Validation Loss: 1.0020\n",
      "Validation loss decreased (1.002165 --> 1.001992).  Saving model ...\n",
      "Epoch 109, Train Loss: 0.9669, Validation Loss: 1.0018\n",
      "Validation loss decreased (1.001992 --> 1.001823).  Saving model ...\n",
      "Epoch 110, Train Loss: 0.9667, Validation Loss: 1.0016\n",
      "Validation loss decreased (1.001823 --> 1.001650).  Saving model ...\n",
      "Epoch 111, Train Loss: 0.9665, Validation Loss: 1.0015\n",
      "Validation loss decreased (1.001650 --> 1.001517).  Saving model ...\n",
      "Epoch 112, Train Loss: 0.9663, Validation Loss: 1.0012\n",
      "Validation loss decreased (1.001517 --> 1.001235).  Saving model ...\n",
      "Epoch 113, Train Loss: 0.9661, Validation Loss: 1.0010\n",
      "Validation loss decreased (1.001235 --> 1.001040).  Saving model ...\n",
      "Epoch 114, Train Loss: 0.9659, Validation Loss: 1.0009\n",
      "Validation loss decreased (1.001040 --> 1.000923).  Saving model ...\n",
      "Epoch 115, Train Loss: 0.9657, Validation Loss: 1.0007\n",
      "Validation loss decreased (1.000923 --> 1.000684).  Saving model ...\n",
      "Epoch 116, Train Loss: 0.9655, Validation Loss: 1.0005\n",
      "Validation loss decreased (1.000684 --> 1.000479).  Saving model ...\n",
      "Epoch 117, Train Loss: 0.9654, Validation Loss: 1.0003\n",
      "Validation loss decreased (1.000479 --> 1.000290).  Saving model ...\n",
      "Epoch 118, Train Loss: 0.9652, Validation Loss: 1.0001\n",
      "Validation loss decreased (1.000290 --> 1.000106).  Saving model ...\n",
      "Epoch 119, Train Loss: 0.9650, Validation Loss: 1.0000\n",
      "Validation loss decreased (1.000106 --> 0.999986).  Saving model ...\n",
      "Epoch 120, Train Loss: 0.9648, Validation Loss: 0.9998\n",
      "Validation loss decreased (0.999986 --> 0.999804).  Saving model ...\n",
      "Epoch 121, Train Loss: 0.9647, Validation Loss: 0.9996\n",
      "Validation loss decreased (0.999804 --> 0.999617).  Saving model ...\n",
      "Epoch 122, Train Loss: 0.9645, Validation Loss: 0.9995\n",
      "Validation loss decreased (0.999617 --> 0.999481).  Saving model ...\n",
      "Epoch 123, Train Loss: 0.9643, Validation Loss: 0.9994\n",
      "Validation loss decreased (0.999481 --> 0.999383).  Saving model ...\n",
      "Epoch 124, Train Loss: 0.9642, Validation Loss: 0.9992\n",
      "Validation loss decreased (0.999383 --> 0.999215).  Saving model ...\n",
      "Epoch 125, Train Loss: 0.9640, Validation Loss: 0.9990\n",
      "Validation loss decreased (0.999215 --> 0.998984).  Saving model ...\n",
      "Epoch 126, Train Loss: 0.9638, Validation Loss: 0.9988\n",
      "Validation loss decreased (0.998984 --> 0.998783).  Saving model ...\n",
      "Epoch 127, Train Loss: 0.9637, Validation Loss: 0.9987\n",
      "Validation loss decreased (0.998783 --> 0.998674).  Saving model ...\n",
      "Epoch 128, Train Loss: 0.9635, Validation Loss: 0.9985\n",
      "Validation loss decreased (0.998674 --> 0.998454).  Saving model ...\n",
      "Epoch 129, Train Loss: 0.9633, Validation Loss: 0.9984\n",
      "Validation loss decreased (0.998454 --> 0.998404).  Saving model ...\n",
      "Epoch 130, Train Loss: 0.9632, Validation Loss: 0.9982\n",
      "Validation loss decreased (0.998404 --> 0.998152).  Saving model ...\n",
      "Epoch 131, Train Loss: 0.9630, Validation Loss: 0.9981\n",
      "Validation loss decreased (0.998152 --> 0.998075).  Saving model ...\n",
      "Epoch 132, Train Loss: 0.9629, Validation Loss: 0.9979\n",
      "Validation loss decreased (0.998075 --> 0.997895).  Saving model ...\n",
      "Epoch 133, Train Loss: 0.9627, Validation Loss: 0.9978\n",
      "Validation loss decreased (0.997895 --> 0.997765).  Saving model ...\n",
      "Epoch 134, Train Loss: 0.9626, Validation Loss: 0.9976\n",
      "Validation loss decreased (0.997765 --> 0.997616).  Saving model ...\n",
      "Epoch 135, Train Loss: 0.9624, Validation Loss: 0.9974\n",
      "Validation loss decreased (0.997616 --> 0.997449).  Saving model ...\n",
      "Epoch 136, Train Loss: 0.9622, Validation Loss: 0.9973\n",
      "Validation loss decreased (0.997449 --> 0.997288).  Saving model ...\n",
      "Epoch 137, Train Loss: 0.9621, Validation Loss: 0.9971\n",
      "Validation loss decreased (0.997288 --> 0.997127).  Saving model ...\n",
      "Epoch 138, Train Loss: 0.9620, Validation Loss: 0.9970\n",
      "Validation loss decreased (0.997127 --> 0.996980).  Saving model ...\n",
      "Epoch 139, Train Loss: 0.9618, Validation Loss: 0.9968\n",
      "Validation loss decreased (0.996980 --> 0.996821).  Saving model ...\n",
      "Epoch 140, Train Loss: 0.9617, Validation Loss: 0.9968\n",
      "Validation loss decreased (0.996821 --> 0.996775).  Saving model ...\n",
      "Epoch 141, Train Loss: 0.9615, Validation Loss: 0.9966\n",
      "Validation loss decreased (0.996775 --> 0.996590).  Saving model ...\n",
      "Epoch 142, Train Loss: 0.9614, Validation Loss: 0.9965\n",
      "Validation loss decreased (0.996590 --> 0.996474).  Saving model ...\n",
      "Epoch 143, Train Loss: 0.9613, Validation Loss: 0.9963\n",
      "Validation loss decreased (0.996474 --> 0.996307).  Saving model ...\n",
      "Epoch 144, Train Loss: 0.9611, Validation Loss: 0.9962\n",
      "Validation loss decreased (0.996307 --> 0.996179).  Saving model ...\n",
      "Epoch 145, Train Loss: 0.9610, Validation Loss: 0.9961\n",
      "Validation loss decreased (0.996179 --> 0.996054).  Saving model ...\n",
      "Epoch 146, Train Loss: 0.9608, Validation Loss: 0.9960\n",
      "Validation loss decreased (0.996054 --> 0.995953).  Saving model ...\n",
      "Epoch 147, Train Loss: 0.9607, Validation Loss: 0.9958\n",
      "Validation loss decreased (0.995953 --> 0.995779).  Saving model ...\n",
      "Epoch 148, Train Loss: 0.9605, Validation Loss: 0.9957\n",
      "Validation loss decreased (0.995779 --> 0.995658).  Saving model ...\n",
      "Epoch 149, Train Loss: 0.9604, Validation Loss: 0.9956\n",
      "Validation loss decreased (0.995658 --> 0.995588).  Saving model ...\n",
      "Epoch 150, Train Loss: 0.9603, Validation Loss: 0.9954\n",
      "Validation loss decreased (0.995588 --> 0.995431).  Saving model ...\n",
      "Epoch 151, Train Loss: 0.9602, Validation Loss: 0.9953\n",
      "Validation loss decreased (0.995431 --> 0.995322).  Saving model ...\n",
      "Epoch 152, Train Loss: 0.9600, Validation Loss: 0.9952\n",
      "Validation loss decreased (0.995322 --> 0.995202).  Saving model ...\n",
      "Epoch 153, Train Loss: 0.9599, Validation Loss: 0.9951\n",
      "Validation loss decreased (0.995202 --> 0.995108).  Saving model ...\n",
      "Epoch 154, Train Loss: 0.9598, Validation Loss: 0.9950\n",
      "Validation loss decreased (0.995108 --> 0.995015).  Saving model ...\n",
      "Epoch 155, Train Loss: 0.9596, Validation Loss: 0.9948\n",
      "Validation loss decreased (0.995015 --> 0.994848).  Saving model ...\n",
      "Epoch 156, Train Loss: 0.9595, Validation Loss: 0.9947\n",
      "Validation loss decreased (0.994848 --> 0.994714).  Saving model ...\n",
      "Epoch 157, Train Loss: 0.9594, Validation Loss: 0.9946\n",
      "Validation loss decreased (0.994714 --> 0.994647).  Saving model ...\n",
      "Epoch 158, Train Loss: 0.9593, Validation Loss: 0.9945\n",
      "Validation loss decreased (0.994647 --> 0.994452).  Saving model ...\n",
      "Epoch 159, Train Loss: 0.9592, Validation Loss: 0.9943\n",
      "Validation loss decreased (0.994452 --> 0.994327).  Saving model ...\n",
      "Epoch 160, Train Loss: 0.9590, Validation Loss: 0.9942\n",
      "Validation loss decreased (0.994327 --> 0.994233).  Saving model ...\n",
      "Epoch 161, Train Loss: 0.9589, Validation Loss: 0.9940\n",
      "Validation loss decreased (0.994233 --> 0.994017).  Saving model ...\n",
      "Epoch 162, Train Loss: 0.9588, Validation Loss: 0.9940\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 163, Train Loss: 0.9587, Validation Loss: 0.9940\n",
      "Validation loss decreased (0.994017 --> 0.993983).  Saving model ...\n",
      "Epoch 164, Train Loss: 0.9586, Validation Loss: 0.9939\n",
      "Validation loss decreased (0.993983 --> 0.993888).  Saving model ...\n",
      "Epoch 165, Train Loss: 0.9585, Validation Loss: 0.9938\n",
      "Validation loss decreased (0.993888 --> 0.993775).  Saving model ...\n",
      "Epoch 166, Train Loss: 0.9583, Validation Loss: 0.9937\n",
      "Validation loss decreased (0.993775 --> 0.993676).  Saving model ...\n",
      "Epoch 167, Train Loss: 0.9582, Validation Loss: 0.9935\n",
      "Validation loss decreased (0.993676 --> 0.993503).  Saving model ...\n",
      "Epoch 168, Train Loss: 0.9581, Validation Loss: 0.9934\n",
      "Validation loss decreased (0.993503 --> 0.993407).  Saving model ...\n",
      "Epoch 169, Train Loss: 0.9580, Validation Loss: 0.9933\n",
      "Validation loss decreased (0.993407 --> 0.993337).  Saving model ...\n",
      "Epoch 170, Train Loss: 0.9579, Validation Loss: 0.9933\n",
      "Validation loss decreased (0.993337 --> 0.993263).  Saving model ...\n",
      "Epoch 171, Train Loss: 0.9578, Validation Loss: 0.9931\n",
      "Validation loss decreased (0.993263 --> 0.993092).  Saving model ...\n",
      "Epoch 172, Train Loss: 0.9577, Validation Loss: 0.9930\n",
      "Validation loss decreased (0.993092 --> 0.993007).  Saving model ...\n",
      "Epoch 173, Train Loss: 0.9576, Validation Loss: 0.9929\n",
      "Validation loss decreased (0.993007 --> 0.992894).  Saving model ...\n",
      "Epoch 174, Train Loss: 0.9575, Validation Loss: 0.9928\n",
      "Validation loss decreased (0.992894 --> 0.992769).  Saving model ...\n",
      "Epoch 175, Train Loss: 0.9573, Validation Loss: 0.9927\n",
      "Validation loss decreased (0.992769 --> 0.992677).  Saving model ...\n",
      "Epoch 176, Train Loss: 0.9572, Validation Loss: 0.9926\n",
      "Validation loss decreased (0.992677 --> 0.992552).  Saving model ...\n",
      "Epoch 177, Train Loss: 0.9571, Validation Loss: 0.9925\n",
      "Validation loss decreased (0.992552 --> 0.992486).  Saving model ...\n",
      "Epoch 178, Train Loss: 0.9570, Validation Loss: 0.9924\n",
      "Validation loss decreased (0.992486 --> 0.992394).  Saving model ...\n",
      "Epoch 179, Train Loss: 0.9569, Validation Loss: 0.9923\n",
      "Validation loss decreased (0.992394 --> 0.992292).  Saving model ...\n",
      "Epoch 180, Train Loss: 0.9568, Validation Loss: 0.9921\n",
      "Validation loss decreased (0.992292 --> 0.992130).  Saving model ...\n",
      "Epoch 181, Train Loss: 0.9567, Validation Loss: 0.9921\n",
      "Validation loss decreased (0.992130 --> 0.992126).  Saving model ...\n",
      "Epoch 182, Train Loss: 0.9566, Validation Loss: 0.9920\n",
      "Validation loss decreased (0.992126 --> 0.992047).  Saving model ...\n",
      "Epoch 183, Train Loss: 0.9565, Validation Loss: 0.9920\n",
      "Validation loss decreased (0.992047 --> 0.991972).  Saving model ...\n",
      "Epoch 184, Train Loss: 0.9564, Validation Loss: 0.9918\n",
      "Validation loss decreased (0.991972 --> 0.991796).  Saving model ...\n",
      "Epoch 185, Train Loss: 0.9563, Validation Loss: 0.9917\n",
      "Validation loss decreased (0.991796 --> 0.991654).  Saving model ...\n",
      "Epoch 186, Train Loss: 0.9562, Validation Loss: 0.9916\n",
      "Validation loss decreased (0.991654 --> 0.991571).  Saving model ...\n",
      "Epoch 187, Train Loss: 0.9561, Validation Loss: 0.9915\n",
      "Validation loss decreased (0.991571 --> 0.991462).  Saving model ...\n",
      "Epoch 188, Train Loss: 0.9560, Validation Loss: 0.9914\n",
      "Validation loss decreased (0.991462 --> 0.991394).  Saving model ...\n",
      "Epoch 189, Train Loss: 0.9559, Validation Loss: 0.9913\n",
      "Validation loss decreased (0.991394 --> 0.991327).  Saving model ...\n",
      "Epoch 190, Train Loss: 0.9558, Validation Loss: 0.9913\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 191, Train Loss: 0.9557, Validation Loss: 0.9912\n",
      "Validation loss decreased (0.991327 --> 0.991215).  Saving model ...\n",
      "Epoch 192, Train Loss: 0.9556, Validation Loss: 0.9912\n",
      "Validation loss decreased (0.991215 --> 0.991165).  Saving model ...\n",
      "Epoch 193, Train Loss: 0.9555, Validation Loss: 0.9911\n",
      "Validation loss decreased (0.991165 --> 0.991068).  Saving model ...\n",
      "Epoch 194, Train Loss: 0.9554, Validation Loss: 0.9910\n",
      "Validation loss decreased (0.991068 --> 0.990967).  Saving model ...\n",
      "Epoch 195, Train Loss: 0.9553, Validation Loss: 0.9909\n",
      "Validation loss decreased (0.990967 --> 0.990869).  Saving model ...\n",
      "Epoch 196, Train Loss: 0.9553, Validation Loss: 0.9908\n",
      "Validation loss decreased (0.990869 --> 0.990774).  Saving model ...\n",
      "Epoch 197, Train Loss: 0.9552, Validation Loss: 0.9907\n",
      "Validation loss decreased (0.990774 --> 0.990652).  Saving model ...\n",
      "Epoch 198, Train Loss: 0.9551, Validation Loss: 0.9906\n",
      "Validation loss decreased (0.990652 --> 0.990565).  Saving model ...\n",
      "Epoch 199, Train Loss: 0.9550, Validation Loss: 0.9905\n",
      "Validation loss decreased (0.990565 --> 0.990522).  Saving model ...\n",
      "Epoch 200, Train Loss: 0.9549, Validation Loss: 0.9905\n",
      "Validation loss decreased (0.990522 --> 0.990477).  Saving model ...\n",
      "Epoch 201, Train Loss: 0.9548, Validation Loss: 0.9903\n",
      "Validation loss decreased (0.990477 --> 0.990341).  Saving model ...\n",
      "Epoch 202, Train Loss: 0.9547, Validation Loss: 0.9903\n",
      "Validation loss decreased (0.990341 --> 0.990311).  Saving model ...\n",
      "Epoch 203, Train Loss: 0.9546, Validation Loss: 0.9902\n",
      "Validation loss decreased (0.990311 --> 0.990222).  Saving model ...\n",
      "Epoch 204, Train Loss: 0.9545, Validation Loss: 0.9901\n",
      "Validation loss decreased (0.990222 --> 0.990141).  Saving model ...\n",
      "Epoch 205, Train Loss: 0.9545, Validation Loss: 0.9900\n",
      "Validation loss decreased (0.990141 --> 0.990041).  Saving model ...\n",
      "Epoch 206, Train Loss: 0.9544, Validation Loss: 0.9899\n",
      "Validation loss decreased (0.990041 --> 0.989887).  Saving model ...\n",
      "Epoch 207, Train Loss: 0.9543, Validation Loss: 0.9899\n",
      "Validation loss decreased (0.989887 --> 0.989876).  Saving model ...\n",
      "Epoch 208, Train Loss: 0.9542, Validation Loss: 0.9898\n",
      "Validation loss decreased (0.989876 --> 0.989810).  Saving model ...\n",
      "Epoch 209, Train Loss: 0.9541, Validation Loss: 0.9897\n",
      "Validation loss decreased (0.989810 --> 0.989707).  Saving model ...\n",
      "Epoch 210, Train Loss: 0.9540, Validation Loss: 0.9896\n",
      "Validation loss decreased (0.989707 --> 0.989573).  Saving model ...\n",
      "Epoch 211, Train Loss: 0.9539, Validation Loss: 0.9895\n",
      "Validation loss decreased (0.989573 --> 0.989485).  Saving model ...\n",
      "Epoch 212, Train Loss: 0.9539, Validation Loss: 0.9895\n",
      "Validation loss decreased (0.989485 --> 0.989456).  Saving model ...\n",
      "Epoch 213, Train Loss: 0.9538, Validation Loss: 0.9894\n",
      "Validation loss decreased (0.989456 --> 0.989433).  Saving model ...\n",
      "Epoch 214, Train Loss: 0.9537, Validation Loss: 0.9893\n",
      "Validation loss decreased (0.989433 --> 0.989316).  Saving model ...\n",
      "Epoch 215, Train Loss: 0.9536, Validation Loss: 0.9892\n",
      "Validation loss decreased (0.989316 --> 0.989213).  Saving model ...\n",
      "Epoch 216, Train Loss: 0.9535, Validation Loss: 0.9891\n",
      "Validation loss decreased (0.989213 --> 0.989125).  Saving model ...\n",
      "Epoch 217, Train Loss: 0.9534, Validation Loss: 0.9891\n",
      "Validation loss decreased (0.989125 --> 0.989084).  Saving model ...\n",
      "Epoch 218, Train Loss: 0.9534, Validation Loss: 0.9890\n",
      "Validation loss decreased (0.989084 --> 0.989021).  Saving model ...\n",
      "Epoch 219, Train Loss: 0.9533, Validation Loss: 0.9890\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 220, Train Loss: 0.9532, Validation Loss: 0.9889\n",
      "Validation loss decreased (0.989021 --> 0.988918).  Saving model ...\n",
      "Epoch 221, Train Loss: 0.9531, Validation Loss: 0.9889\n",
      "Validation loss decreased (0.988918 --> 0.988853).  Saving model ...\n",
      "Epoch 222, Train Loss: 0.9531, Validation Loss: 0.9887\n",
      "Validation loss decreased (0.988853 --> 0.988739).  Saving model ...\n",
      "Epoch 223, Train Loss: 0.9530, Validation Loss: 0.9886\n",
      "Validation loss decreased (0.988739 --> 0.988647).  Saving model ...\n",
      "Epoch 224, Train Loss: 0.9529, Validation Loss: 0.9886\n",
      "Validation loss decreased (0.988647 --> 0.988624).  Saving model ...\n",
      "Epoch 225, Train Loss: 0.9528, Validation Loss: 0.9886\n",
      "Validation loss decreased (0.988624 --> 0.988607).  Saving model ...\n",
      "Epoch 226, Train Loss: 0.9528, Validation Loss: 0.9886\n",
      "Validation loss decreased (0.988607 --> 0.988592).  Saving model ...\n",
      "Epoch 227, Train Loss: 0.9527, Validation Loss: 0.9885\n",
      "Validation loss decreased (0.988592 --> 0.988458).  Saving model ...\n",
      "Epoch 228, Train Loss: 0.9526, Validation Loss: 0.9885\n",
      "Validation loss decreased (0.988458 --> 0.988453).  Saving model ...\n",
      "Epoch 229, Train Loss: 0.9525, Validation Loss: 0.9883\n",
      "Validation loss decreased (0.988453 --> 0.988309).  Saving model ...\n",
      "Epoch 230, Train Loss: 0.9525, Validation Loss: 0.9883\n",
      "Validation loss decreased (0.988309 --> 0.988282).  Saving model ...\n",
      "Epoch 231, Train Loss: 0.9524, Validation Loss: 0.9881\n",
      "Validation loss decreased (0.988282 --> 0.988145).  Saving model ...\n",
      "Epoch 232, Train Loss: 0.9523, Validation Loss: 0.9880\n",
      "Validation loss decreased (0.988145 --> 0.988037).  Saving model ...\n",
      "Epoch 233, Train Loss: 0.9522, Validation Loss: 0.9880\n",
      "Validation loss decreased (0.988037 --> 0.988005).  Saving model ...\n",
      "Epoch 234, Train Loss: 0.9522, Validation Loss: 0.9879\n",
      "Validation loss decreased (0.988005 --> 0.987874).  Saving model ...\n",
      "Epoch 235, Train Loss: 0.9521, Validation Loss: 0.9878\n",
      "Validation loss decreased (0.987874 --> 0.987843).  Saving model ...\n",
      "Epoch 236, Train Loss: 0.9520, Validation Loss: 0.9879\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 237, Train Loss: 0.9519, Validation Loss: 0.9877\n",
      "Validation loss decreased (0.987843 --> 0.987676).  Saving model ...\n",
      "Epoch 238, Train Loss: 0.9519, Validation Loss: 0.9877\n",
      "Validation loss decreased (0.987676 --> 0.987665).  Saving model ...\n",
      "Epoch 239, Train Loss: 0.9518, Validation Loss: 0.9876\n",
      "Validation loss decreased (0.987665 --> 0.987620).  Saving model ...\n",
      "Epoch 240, Train Loss: 0.9517, Validation Loss: 0.9875\n",
      "Validation loss decreased (0.987620 --> 0.987542).  Saving model ...\n",
      "Epoch 241, Train Loss: 0.9517, Validation Loss: 0.9874\n",
      "Validation loss decreased (0.987542 --> 0.987390).  Saving model ...\n",
      "Epoch 242, Train Loss: 0.9516, Validation Loss: 0.9874\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 243, Train Loss: 0.9515, Validation Loss: 0.9874\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 244, Train Loss: 0.9515, Validation Loss: 0.9874\n",
      "Validation loss decreased (0.987390 --> 0.987369).  Saving model ...\n",
      "Epoch 245, Train Loss: 0.9514, Validation Loss: 0.9873\n",
      "Validation loss decreased (0.987369 --> 0.987253).  Saving model ...\n",
      "Epoch 246, Train Loss: 0.9513, Validation Loss: 0.9872\n",
      "Validation loss decreased (0.987253 --> 0.987239).  Saving model ...\n",
      "Epoch 247, Train Loss: 0.9513, Validation Loss: 0.9872\n",
      "Validation loss decreased (0.987239 --> 0.987208).  Saving model ...\n",
      "Epoch 248, Train Loss: 0.9512, Validation Loss: 0.9872\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 249, Train Loss: 0.9511, Validation Loss: 0.9870\n",
      "Validation loss decreased (0.987208 --> 0.987018).  Saving model ...\n",
      "Epoch 250, Train Loss: 0.9511, Validation Loss: 0.9870\n",
      "Validation loss decreased (0.987018 --> 0.986990).  Saving model ...\n",
      "Epoch 251, Train Loss: 0.9510, Validation Loss: 0.9869\n",
      "Validation loss decreased (0.986990 --> 0.986914).  Saving model ...\n",
      "Epoch 252, Train Loss: 0.9509, Validation Loss: 0.9869\n",
      "Validation loss decreased (0.986914 --> 0.986906).  Saving model ...\n",
      "Epoch 253, Train Loss: 0.9508, Validation Loss: 0.9869\n",
      "Validation loss decreased (0.986906 --> 0.986885).  Saving model ...\n",
      "Epoch 254, Train Loss: 0.9508, Validation Loss: 0.9868\n",
      "Validation loss decreased (0.986885 --> 0.986803).  Saving model ...\n",
      "Epoch 255, Train Loss: 0.9507, Validation Loss: 0.9867\n",
      "Validation loss decreased (0.986803 --> 0.986704).  Saving model ...\n",
      "Epoch 256, Train Loss: 0.9507, Validation Loss: 0.9866\n",
      "Validation loss decreased (0.986704 --> 0.986582).  Saving model ...\n",
      "Epoch 257, Train Loss: 0.9506, Validation Loss: 0.9866\n",
      "Validation loss decreased (0.986582 --> 0.986580).  Saving model ...\n",
      "Epoch 258, Train Loss: 0.9505, Validation Loss: 0.9866\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 259, Train Loss: 0.9505, Validation Loss: 0.9865\n",
      "Validation loss decreased (0.986580 --> 0.986524).  Saving model ...\n",
      "Epoch 260, Train Loss: 0.9504, Validation Loss: 0.9864\n",
      "Validation loss decreased (0.986524 --> 0.986422).  Saving model ...\n",
      "Epoch 261, Train Loss: 0.9503, Validation Loss: 0.9864\n",
      "Validation loss decreased (0.986422 --> 0.986360).  Saving model ...\n",
      "Epoch 262, Train Loss: 0.9503, Validation Loss: 0.9863\n",
      "Validation loss decreased (0.986360 --> 0.986334).  Saving model ...\n",
      "Epoch 263, Train Loss: 0.9502, Validation Loss: 0.9862\n",
      "Validation loss decreased (0.986334 --> 0.986227).  Saving model ...\n",
      "Epoch 264, Train Loss: 0.9502, Validation Loss: 0.9862\n",
      "Validation loss decreased (0.986227 --> 0.986207).  Saving model ...\n",
      "Epoch 265, Train Loss: 0.9501, Validation Loss: 0.9861\n",
      "Validation loss decreased (0.986207 --> 0.986126).  Saving model ...\n",
      "Epoch 266, Train Loss: 0.9500, Validation Loss: 0.9861\n",
      "Validation loss decreased (0.986126 --> 0.986119).  Saving model ...\n",
      "Epoch 267, Train Loss: 0.9500, Validation Loss: 0.9862\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 268, Train Loss: 0.9499, Validation Loss: 0.9860\n",
      "Validation loss decreased (0.986119 --> 0.986003).  Saving model ...\n",
      "Epoch 269, Train Loss: 0.9498, Validation Loss: 0.9860\n",
      "Validation loss decreased (0.986003 --> 0.985999).  Saving model ...\n",
      "Epoch 270, Train Loss: 0.9498, Validation Loss: 0.9859\n",
      "Validation loss decreased (0.985999 --> 0.985904).  Saving model ...\n",
      "Epoch 271, Train Loss: 0.9497, Validation Loss: 0.9859\n",
      "Validation loss decreased (0.985904 --> 0.985862).  Saving model ...\n",
      "Epoch 272, Train Loss: 0.9497, Validation Loss: 0.9858\n",
      "Validation loss decreased (0.985862 --> 0.985819).  Saving model ...\n",
      "Epoch 273, Train Loss: 0.9496, Validation Loss: 0.9858\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 274, Train Loss: 0.9496, Validation Loss: 0.9858\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 275, Train Loss: 0.9495, Validation Loss: 0.9858\n",
      "Validation loss decreased (0.985819 --> 0.985756).  Saving model ...\n",
      "Epoch 276, Train Loss: 0.9494, Validation Loss: 0.9857\n",
      "Validation loss decreased (0.985756 --> 0.985655).  Saving model ...\n",
      "Epoch 277, Train Loss: 0.9494, Validation Loss: 0.9856\n",
      "Validation loss decreased (0.985655 --> 0.985609).  Saving model ...\n",
      "Epoch 278, Train Loss: 0.9493, Validation Loss: 0.9855\n",
      "Validation loss decreased (0.985609 --> 0.985527).  Saving model ...\n",
      "Epoch 279, Train Loss: 0.9493, Validation Loss: 0.9854\n",
      "Validation loss decreased (0.985527 --> 0.985441).  Saving model ...\n",
      "Epoch 280, Train Loss: 0.9492, Validation Loss: 0.9854\n",
      "Validation loss decreased (0.985441 --> 0.985404).  Saving model ...\n",
      "Epoch 281, Train Loss: 0.9492, Validation Loss: 0.9855\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 282, Train Loss: 0.9491, Validation Loss: 0.9854\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 283, Train Loss: 0.9490, Validation Loss: 0.9853\n",
      "Validation loss decreased (0.985404 --> 0.985344).  Saving model ...\n",
      "Epoch 284, Train Loss: 0.9490, Validation Loss: 0.9853\n",
      "Validation loss decreased (0.985344 --> 0.985308).  Saving model ...\n",
      "Epoch 285, Train Loss: 0.9489, Validation Loss: 0.9852\n",
      "Validation loss decreased (0.985308 --> 0.985179).  Saving model ...\n",
      "Epoch 286, Train Loss: 0.9489, Validation Loss: 0.9852\n",
      "Validation loss decreased (0.985179 --> 0.985172).  Saving model ...\n",
      "Epoch 287, Train Loss: 0.9488, Validation Loss: 0.9850\n",
      "Validation loss decreased (0.985172 --> 0.985044).  Saving model ...\n",
      "Epoch 288, Train Loss: 0.9488, Validation Loss: 0.9850\n",
      "Validation loss decreased (0.985044 --> 0.985032).  Saving model ...\n",
      "Epoch 289, Train Loss: 0.9487, Validation Loss: 0.9850\n",
      "Validation loss decreased (0.985032 --> 0.984995).  Saving model ...\n",
      "Epoch 290, Train Loss: 0.9486, Validation Loss: 0.9851\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 291, Train Loss: 0.9486, Validation Loss: 0.9850\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 292, Train Loss: 0.9485, Validation Loss: 0.9850\n",
      "Validation loss decreased (0.984995 --> 0.984988).  Saving model ...\n",
      "Epoch 293, Train Loss: 0.9485, Validation Loss: 0.9850\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 294, Train Loss: 0.9484, Validation Loss: 0.9849\n",
      "Validation loss decreased (0.984988 --> 0.984944).  Saving model ...\n",
      "Epoch 295, Train Loss: 0.9484, Validation Loss: 0.9849\n",
      "Validation loss decreased (0.984944 --> 0.984894).  Saving model ...\n",
      "Epoch 296, Train Loss: 0.9483, Validation Loss: 0.9848\n",
      "Validation loss decreased (0.984894 --> 0.984799).  Saving model ...\n",
      "Epoch 297, Train Loss: 0.9483, Validation Loss: 0.9847\n",
      "Validation loss decreased (0.984799 --> 0.984701).  Saving model ...\n",
      "Epoch 298, Train Loss: 0.9482, Validation Loss: 0.9847\n",
      "Validation loss decreased (0.984701 --> 0.984693).  Saving model ...\n",
      "Epoch 299, Train Loss: 0.9482, Validation Loss: 0.9846\n",
      "Validation loss decreased (0.984693 --> 0.984587).  Saving model ...\n",
      "Epoch 300, Train Loss: 0.9481, Validation Loss: 0.9845\n",
      "Validation loss decreased (0.984587 --> 0.984524).  Saving model ...\n",
      "Epoch 301, Train Loss: 0.9480, Validation Loss: 0.9845\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 302, Train Loss: 0.9480, Validation Loss: 0.9845\n",
      "Validation loss decreased (0.984524 --> 0.984507).  Saving model ...\n",
      "Epoch 303, Train Loss: 0.9479, Validation Loss: 0.9844\n",
      "Validation loss decreased (0.984507 --> 0.984392).  Saving model ...\n",
      "Epoch 304, Train Loss: 0.9479, Validation Loss: 0.9844\n",
      "Validation loss decreased (0.984392 --> 0.984386).  Saving model ...\n",
      "Epoch 305, Train Loss: 0.9478, Validation Loss: 0.9843\n",
      "Validation loss decreased (0.984386 --> 0.984336).  Saving model ...\n",
      "Epoch 306, Train Loss: 0.9478, Validation Loss: 0.9843\n",
      "Validation loss decreased (0.984336 --> 0.984304).  Saving model ...\n",
      "Epoch 307, Train Loss: 0.9477, Validation Loss: 0.9843\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 308, Train Loss: 0.9477, Validation Loss: 0.9842\n",
      "Validation loss decreased (0.984304 --> 0.984224).  Saving model ...\n",
      "Epoch 309, Train Loss: 0.9477, Validation Loss: 0.9842\n",
      "Validation loss decreased (0.984224 --> 0.984194).  Saving model ...\n",
      "Epoch 310, Train Loss: 0.9476, Validation Loss: 0.9841\n",
      "Validation loss decreased (0.984194 --> 0.984081).  Saving model ...\n",
      "Epoch 311, Train Loss: 0.9475, Validation Loss: 0.9841\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 312, Train Loss: 0.9475, Validation Loss: 0.9840\n",
      "Validation loss decreased (0.984081 --> 0.984048).  Saving model ...\n",
      "Epoch 313, Train Loss: 0.9474, Validation Loss: 0.9841\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 314, Train Loss: 0.9474, Validation Loss: 0.9840\n",
      "Validation loss decreased (0.984048 --> 0.984041).  Saving model ...\n",
      "Epoch 315, Train Loss: 0.9473, Validation Loss: 0.9840\n",
      "Validation loss decreased (0.984041 --> 0.983952).  Saving model ...\n",
      "Epoch 316, Train Loss: 0.9473, Validation Loss: 0.9839\n",
      "Validation loss decreased (0.983952 --> 0.983924).  Saving model ...\n",
      "Epoch 317, Train Loss: 0.9472, Validation Loss: 0.9838\n",
      "Validation loss decreased (0.983924 --> 0.983811).  Saving model ...\n",
      "Epoch 318, Train Loss: 0.9472, Validation Loss: 0.9838\n",
      "Validation loss decreased (0.983811 --> 0.983807).  Saving model ...\n",
      "Epoch 319, Train Loss: 0.9472, Validation Loss: 0.9838\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 320, Train Loss: 0.9471, Validation Loss: 0.9838\n",
      "Validation loss decreased (0.983807 --> 0.983795).  Saving model ...\n",
      "Epoch 321, Train Loss: 0.9471, Validation Loss: 0.9838\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 322, Train Loss: 0.9470, Validation Loss: 0.9838\n",
      "Validation loss decreased (0.983795 --> 0.983775).  Saving model ...\n",
      "Epoch 323, Train Loss: 0.9470, Validation Loss: 0.9837\n",
      "Validation loss decreased (0.983775 --> 0.983742).  Saving model ...\n",
      "Epoch 324, Train Loss: 0.9469, Validation Loss: 0.9837\n",
      "Validation loss decreased (0.983742 --> 0.983657).  Saving model ...\n",
      "Epoch 325, Train Loss: 0.9469, Validation Loss: 0.9836\n",
      "Validation loss decreased (0.983657 --> 0.983604).  Saving model ...\n",
      "Epoch 326, Train Loss: 0.9468, Validation Loss: 0.9835\n",
      "Validation loss decreased (0.983604 --> 0.983529).  Saving model ...\n",
      "Epoch 327, Train Loss: 0.9468, Validation Loss: 0.9836\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 328, Train Loss: 0.9467, Validation Loss: 0.9835\n",
      "Validation loss decreased (0.983529 --> 0.983487).  Saving model ...\n",
      "Epoch 329, Train Loss: 0.9467, Validation Loss: 0.9835\n",
      "Validation loss decreased (0.983487 --> 0.983470).  Saving model ...\n",
      "Epoch 330, Train Loss: 0.9466, Validation Loss: 0.9835\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 331, Train Loss: 0.9466, Validation Loss: 0.9834\n",
      "Validation loss decreased (0.983470 --> 0.983375).  Saving model ...\n",
      "Epoch 332, Train Loss: 0.9465, Validation Loss: 0.9834\n",
      "Validation loss decreased (0.983375 --> 0.983355).  Saving model ...\n",
      "Epoch 333, Train Loss: 0.9465, Validation Loss: 0.9834\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 334, Train Loss: 0.9464, Validation Loss: 0.9833\n",
      "Validation loss decreased (0.983355 --> 0.983302).  Saving model ...\n",
      "Epoch 335, Train Loss: 0.9464, Validation Loss: 0.9832\n",
      "Validation loss decreased (0.983302 --> 0.983245).  Saving model ...\n",
      "Epoch 336, Train Loss: 0.9464, Validation Loss: 0.9832\n",
      "Validation loss decreased (0.983245 --> 0.983222).  Saving model ...\n",
      "Epoch 337, Train Loss: 0.9463, Validation Loss: 0.9832\n",
      "Validation loss decreased (0.983222 --> 0.983210).  Saving model ...\n",
      "Epoch 338, Train Loss: 0.9463, Validation Loss: 0.9832\n",
      "Validation loss decreased (0.983210 --> 0.983208).  Saving model ...\n",
      "Epoch 339, Train Loss: 0.9462, Validation Loss: 0.9831\n",
      "Validation loss decreased (0.983208 --> 0.983124).  Saving model ...\n",
      "Epoch 340, Train Loss: 0.9462, Validation Loss: 0.9831\n",
      "Validation loss decreased (0.983124 --> 0.983070).  Saving model ...\n",
      "Epoch 341, Train Loss: 0.9461, Validation Loss: 0.9830\n",
      "Validation loss decreased (0.983070 --> 0.982992).  Saving model ...\n",
      "Epoch 342, Train Loss: 0.9461, Validation Loss: 0.9829\n",
      "Validation loss decreased (0.982992 --> 0.982918).  Saving model ...\n",
      "Epoch 343, Train Loss: 0.9461, Validation Loss: 0.9829\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 344, Train Loss: 0.9460, Validation Loss: 0.9829\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 345, Train Loss: 0.9460, Validation Loss: 0.9830\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 346, Train Loss: 0.9459, Validation Loss: 0.9829\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 347, Train Loss: 0.9459, Validation Loss: 0.9828\n",
      "Validation loss decreased (0.982918 --> 0.982832).  Saving model ...\n",
      "Epoch 348, Train Loss: 0.9458, Validation Loss: 0.9829\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 349, Train Loss: 0.9458, Validation Loss: 0.9829\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 350, Train Loss: 0.9457, Validation Loss: 0.9828\n",
      "Validation loss decreased (0.982832 --> 0.982819).  Saving model ...\n",
      "Epoch 351, Train Loss: 0.9457, Validation Loss: 0.9828\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 352, Train Loss: 0.9456, Validation Loss: 0.9828\n",
      "Validation loss decreased (0.982819 --> 0.982797).  Saving model ...\n",
      "Epoch 353, Train Loss: 0.9456, Validation Loss: 0.9827\n",
      "Validation loss decreased (0.982797 --> 0.982694).  Saving model ...\n",
      "Epoch 354, Train Loss: 0.9456, Validation Loss: 0.9827\n",
      "Validation loss decreased (0.982694 --> 0.982674).  Saving model ...\n",
      "Epoch 355, Train Loss: 0.9455, Validation Loss: 0.9827\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 356, Train Loss: 0.9455, Validation Loss: 0.9826\n",
      "Validation loss decreased (0.982674 --> 0.982619).  Saving model ...\n",
      "Epoch 357, Train Loss: 0.9454, Validation Loss: 0.9825\n",
      "Validation loss decreased (0.982619 --> 0.982523).  Saving model ...\n",
      "Epoch 358, Train Loss: 0.9454, Validation Loss: 0.9825\n",
      "Validation loss decreased (0.982523 --> 0.982515).  Saving model ...\n",
      "Epoch 359, Train Loss: 0.9454, Validation Loss: 0.9825\n",
      "Validation loss decreased (0.982515 --> 0.982486).  Saving model ...\n",
      "Epoch 360, Train Loss: 0.9453, Validation Loss: 0.9825\n",
      "Validation loss decreased (0.982486 --> 0.982483).  Saving model ...\n",
      "Epoch 361, Train Loss: 0.9453, Validation Loss: 0.9824\n",
      "Validation loss decreased (0.982483 --> 0.982404).  Saving model ...\n",
      "Epoch 362, Train Loss: 0.9452, Validation Loss: 0.9824\n",
      "Validation loss decreased (0.982404 --> 0.982355).  Saving model ...\n",
      "Epoch 363, Train Loss: 0.9452, Validation Loss: 0.9824\n",
      "Validation loss decreased (0.982355 --> 0.982353).  Saving model ...\n",
      "Epoch 364, Train Loss: 0.9451, Validation Loss: 0.9823\n",
      "Validation loss decreased (0.982353 --> 0.982314).  Saving model ...\n",
      "Epoch 365, Train Loss: 0.9451, Validation Loss: 0.9824\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 366, Train Loss: 0.9451, Validation Loss: 0.9823\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 367, Train Loss: 0.9450, Validation Loss: 0.9823\n",
      "Validation loss decreased (0.982314 --> 0.982304).  Saving model ...\n",
      "Epoch 368, Train Loss: 0.9450, Validation Loss: 0.9822\n",
      "Validation loss decreased (0.982304 --> 0.982244).  Saving model ...\n",
      "Epoch 369, Train Loss: 0.9449, Validation Loss: 0.9822\n",
      "Validation loss decreased (0.982244 --> 0.982227).  Saving model ...\n",
      "Epoch 370, Train Loss: 0.9449, Validation Loss: 0.9822\n",
      "Validation loss decreased (0.982227 --> 0.982168).  Saving model ...\n",
      "Epoch 371, Train Loss: 0.9449, Validation Loss: 0.9821\n",
      "Validation loss decreased (0.982168 --> 0.982092).  Saving model ...\n",
      "Epoch 372, Train Loss: 0.9448, Validation Loss: 0.9822\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 373, Train Loss: 0.9448, Validation Loss: 0.9821\n",
      "Validation loss decreased (0.982092 --> 0.982072).  Saving model ...\n",
      "Epoch 374, Train Loss: 0.9447, Validation Loss: 0.9821\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 375, Train Loss: 0.9447, Validation Loss: 0.9821\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 376, Train Loss: 0.9447, Validation Loss: 0.9820\n",
      "Validation loss decreased (0.982072 --> 0.981964).  Saving model ...\n",
      "Epoch 377, Train Loss: 0.9446, Validation Loss: 0.9819\n",
      "Validation loss decreased (0.981964 --> 0.981936).  Saving model ...\n",
      "Epoch 378, Train Loss: 0.9446, Validation Loss: 0.9820\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 379, Train Loss: 0.9446, Validation Loss: 0.9820\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 380, Train Loss: 0.9445, Validation Loss: 0.9819\n",
      "Validation loss decreased (0.981936 --> 0.981927).  Saving model ...\n",
      "Epoch 381, Train Loss: 0.9445, Validation Loss: 0.9819\n",
      "Validation loss decreased (0.981927 --> 0.981892).  Saving model ...\n",
      "Epoch 382, Train Loss: 0.9445, Validation Loss: 0.9817\n",
      "Validation loss decreased (0.981892 --> 0.981737).  Saving model ...\n",
      "Epoch 383, Train Loss: 0.9444, Validation Loss: 0.9817\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 384, Train Loss: 0.9444, Validation Loss: 0.9817\n",
      "Validation loss decreased (0.981737 --> 0.981682).  Saving model ...\n",
      "Epoch 385, Train Loss: 0.9443, Validation Loss: 0.9817\n",
      "Validation loss decreased (0.981682 --> 0.981650).  Saving model ...\n",
      "Epoch 386, Train Loss: 0.9443, Validation Loss: 0.9817\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 387, Train Loss: 0.9443, Validation Loss: 0.9817\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 388, Train Loss: 0.9442, Validation Loss: 0.9817\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 389, Train Loss: 0.9442, Validation Loss: 0.9816\n",
      "Validation loss decreased (0.981650 --> 0.981605).  Saving model ...\n",
      "Epoch 390, Train Loss: 0.9441, Validation Loss: 0.9815\n",
      "Validation loss decreased (0.981605 --> 0.981524).  Saving model ...\n",
      "Epoch 391, Train Loss: 0.9441, Validation Loss: 0.9815\n",
      "Validation loss decreased (0.981524 --> 0.981473).  Saving model ...\n",
      "Epoch 392, Train Loss: 0.9441, Validation Loss: 0.9815\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 393, Train Loss: 0.9440, Validation Loss: 0.9815\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 394, Train Loss: 0.9440, Validation Loss: 0.9815\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 395, Train Loss: 0.9440, Validation Loss: 0.9815\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 396, Train Loss: 0.9439, Validation Loss: 0.9815\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def training(epochs):\n",
    "    loss_ = []\n",
    "    valoss_ = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "\n",
    "        # Training process\n",
    "        for data in trainloader:\n",
    "            model.train()\n",
    "            inputs = data                                                           # Assuming data is the input tensor\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predicted_outputs = model(inputs)\n",
    "            train_loss = criterion(predicted_outputs, inputs)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += train_loss.item()\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = running_train_loss / len(trainloader)\n",
    "        loss_.append(avg_train_loss)\n",
    "        \n",
    "        # Validation process\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for data in validloader:\n",
    "                inputs = data\n",
    "                inputs = inputs.to(device)\n",
    "                predicted_outputs = model(inputs)\n",
    "                val_loss = criterion(predicted_outputs, inputs)\n",
    "                running_vall_loss += val_loss.item()\n",
    "\n",
    "            avg_val_loss = running_vall_loss / len(validloader)\n",
    "            valoss_.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    " \n",
    "    # Save model\n",
    "    saveModel()\n",
    "    return loss_, valoss_  # Return the collected loss values for training and validation\n",
    "\n",
    "# Now when you call training, it will return the lists to be unpacked.\n",
    "loss_, valoss_ = training(epochs=400)  # Set the appropriate number of epochs\n",
    "\n",
    "# Loss visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_, label='Train Loss')\n",
    "plt.plot(valoss_, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_lstm_autoencoder_AR(2).pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL0 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "l = 12 # 윈도우 사이즈\n",
    "\n",
    "def ar2gen(phi1, phi2, psi, delta,gamma, length) :\n",
    "\n",
    "    e = np.random.normal(loc=0, scale = 1,size = length)\n",
    "    sigma = math.sqrt((1 - phi2) / ((1 + phi2) * (1 - phi2 - phi1) * (1 - phi2 + phi1)))\n",
    "    x = np.array(np.repeat(0, length), dtype= np.float64)\n",
    "    x[0] = e[0]\n",
    "    z = np.array(np.repeat(0, length), dtype=np.float64)\n",
    "\n",
    "    for i in range(2, psi):\n",
    "        x[i] = phi1 * x[i - 1] + phi2 * x[i - 2] + e[i]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(x)):\n",
    "        x[i] = phi1 * x[i - 1] + phi2 * x[i - 2] + gamma*e[i]\n",
    "        z[i] = x[i]\n",
    "    for i in range(psi,len(z)):\n",
    "        z[i] = z[i] + delta * sigma\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl(phi1, phi2, delta, gamma, run, length, cl) :\n",
    "    rl = np.array([], dtype=np.float64)\n",
    "    \n",
    "    for i in tqdm(range(run)) :\n",
    "        y = ar2gen(phi1, phi2, psi=l-1, delta=delta, gamma = gamma,length=length)\n",
    "        a = np.array([length-l])\n",
    "        x = np.zeros(shape=(length-l, l))\n",
    "        for j in range(length-l):\n",
    "            x[j] = y[j: j + l]\n",
    "        x = torch.FloatTensor(x).to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j in range(0,len(x)):\n",
    "                input = x[[j]]\n",
    "\n",
    "                output = model(input)\n",
    "                \n",
    "                mse_loss = nn.MSELoss()\n",
    "                loss = mse_loss(output[0], input[0])\n",
    "\n",
    "                if loss > cl :\n",
    "\n",
    "                    a = np.array([j + 1])\n",
    "                    break\n",
    "                elif j == len(x):\n",
    "                    a = len(x)\n",
    "\n",
    "            rl = np.append(rl,a)\n",
    "\n",
    "    arl = np.mean(rl)\n",
    "    return arl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARL1 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arl1(phi1, phi2, run, length, cl):\n",
    "    a5 = arl(phi1, phi2, 0.5, 1, run, length, cl)\n",
    "    a1 = arl(phi1, phi2, 1, 1, run, length, cl)\n",
    "    a2 = arl(phi1, phi2, 2, 1, run, length, cl)\n",
    "    a3 = arl(phi1, phi2, 3, 1, run, length, cl)\n",
    "    b5 = arl(phi1, phi2, 0.5, 1.5,run, length, cl)\n",
    "    b1 = arl(phi1, phi2, 1, 1.5, run, length, cl)\n",
    "    b2 = arl(phi1, phi2, 2, 1.5, run, length, cl)\n",
    "    b3 = arl(phi1, phi2, 3, 1, run, length, cl)\n",
    "    c1 = arl(phi1, phi2, 0, 1.5, run, length, cl)\n",
    "    c2 = arl(phi1, phi2, 0, 2, run, length, cl)\n",
    "    c3 = arl(phi1, phi2, 0, 3, run, length, cl)\n",
    "    print(f'0.5: {a5}, 1:{a1},2:{a2},3:{a3}')\n",
    "    print(f'0.5:{b5},1:{b1},2:{b2},3:{b3}')\n",
    "    print(f'1.5:{c1},2:{c2},3:{c3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1) phi1 = 0, phi2 = 0.1 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL0 (threshold 임의추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/python3119/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.582"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold 2.13\n",
    "arl(phi1=0, phi2=0.1, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 2.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:02<00:00,  5.48it/s]\n",
      "100%|██████████| 1000/1000 [01:44<00:00,  9.53it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 68.26it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 310.08it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 104.21it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 119.81it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 197.40it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 318.66it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 93.06it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 205.83it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 328.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5: 349.642, 1:188.634,2:24.714,3:3.936\n",
      "0.5:15.39,1:13.097,2:7.255,3:3.783\n",
      "1.5:17.593,2:7.128,3:3.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arl1(phi1=0, phi2=0.1, run=1000, length=1000, cl= 2.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2) phi1 = 0.25, phi2 = 0.2 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL0 (threshold 임의추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/python3119/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# threshold 2.277\n",
    "arl(phi1=0.25, phi2=0.2, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 2.277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arl1(phi1=0.25, phi2=0.2, run=1000, length=1000, cl= 2.277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3) phi1 = 0.4, phi2 = 0.3 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL0 (threshold 임의추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/python3119/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# threshold 2.94\n",
    "arl(phi1=0.4, phi2=0.3, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 2.94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arl1(phi1=0.4, phi2=0.3, run=1000, length=1000, cl= 2.94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4) phi1 = 0.6, phi2 = 0.2 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL0 (threshold 임의추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/python3119/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# threshold 3.98\n",
    "arl(phi1=0.6, phi2=0.2, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 3.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arl1(phi1=0.6, phi2=0.2, run=1000, length=1000, cl= 3.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 5) phi1 = 0.8, phi2 = 0.1 일 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL0 (threshold 임의추정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/python3119/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423.582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# threshold 5.9\n",
    "arl(phi1=0.8, phi2=0.1, delta = 0, gamma = 1, run = 1000, length = 1000, cl = 5.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arl1(phi1=0.8, phi2=0.1, run=1000, length=1000, cl= 5.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3119",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
