{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"im1rpNMQGuPs"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from pytorchtools import EarlyStopping\n","import math\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"cINq7WARGuPv"},"source":["#### Numpy & Python Version 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muV0XNxcGuPw","outputId":"3714b668-caf7-4d6b-89b9-28e302213a06"},"outputs":[{"name":"stdout","output_type":"stream","text":["NumPy 버전: 1.24.3\n","Python 버전: 3.11.9\n"]}],"source":["import numpy as np\n","import platform\n","\n","# NumPy 버전 확인\n","numpy_version = np.__version__\n","\n","# Python 버전 확인\n","python_version = platform.python_version()\n","\n","print(f\"NumPy 버전: {numpy_version}\")\n","print(f\"Python 버전: {python_version}\")"]},{"cell_type":"markdown","metadata":{"id":"Os-M5uCHGuPx"},"source":["#### CUDA 사용 및 EarlyStopping 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sudrGJlgGuPx","outputId":"832c4219-ddc0-4371-c6c0-2a934ec1ff8f"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMjrv6MNGuPy","outputId":"c7dac875-f9e3-471e-b7b8-66ddc614e46e"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)\n","early_stopping = EarlyStopping(patience = 5, verbose = True)"]},{"cell_type":"markdown","metadata":{"id":"WUlMqpguGuPy"},"source":["#### 모델 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJOEhzPLGuPz"},"outputs":[],"source":["def saveModel():\n","    torch.save(model.state_dict(), 'F:/응용통계학과/4학년 1학기/학부연구생/LSTM/논문 구현/model_lstmautoencoder.pt') # 모델의 학습된 매개변수 파일에 저장"]},{"cell_type":"markdown","metadata":{"id":"tCy6W19UGuPz"},"source":["#### 하이퍼 파라미터 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtJC9DtTGuPz"},"outputs":[],"source":["# 하이퍼파라미터 설정\n","length = 12            # 윈도우 사이즈 (생성할 시계열 데이터의 길이)\n","trainrun = 250       # 생성할 학습 데이터 시퀀스의 수\n","validrun = 125         # 생성할 검증 데이터 시퀀스의 수\n","\n","# 시계열 데이터 생성을 위한 매개변수\n","# 자기상관계수 (phi값을 0.25, 0.5, 0.75, 0.95별로 생성)\n","phi1 = np.array([0.25, 0.5, 0.75, 0.95])\n","\n","# 변화율 크기\n","psi1 = 0\n","\n","# 공정의 수준 변화율 (delta)\n","de1 = 0\n","\n","# 공정의 분산 변화율 (gamma)\n","ga = 1"]},{"cell_type":"markdown","metadata":{"id":"ar9c7zFHGuP0"},"source":["#### 시계열 데이터 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uJIyeQ1GuP0"},"outputs":[],"source":["np.random.seed(1)\n","\n","# AR(1) 시계열 데이터 생성 함수\n","def ar(ar1, delta, gamma, psi, length, run):\n","    # 초기 설정\n","    y = np.zeros(shape=(run, length))                           # 생성될 시계열 데이터를 저장할 빈 배열을 초기화. 배열의 크기는 (생성할 데이터 시퀀스의 수, 각 시퀀스의 길이)\n","    sigma = math.sqrt(1 / (1 - pow(ar1, 2)))                    # AR(1)모델의 표준 편차\n","\n","    # 데이터 시퀀스 생성\n","    for j in range(0, run):                                     # 각 run 마다 랜덤 노이즈(e)를 정규분포에서 추출하여 시계열의 기본 노이즈 생성 (과적합 방지 차원)\n","        e = np.random.normal(loc=0, scale=1, size=length)\n","        x = np.zeros(length)\n","\n","        x[0] = e[0]                                             # x 배열 초기화하고, 첫 번째 시점의 값은 첫 번째 노이즈 값으로 설정 (시계열의 시작점에서 발생할 수 있는 임의성 반영 및 자기상관 구조 구현)\n","\n","        # psi 시점 이전의 데이터 생성\n","        for i in range(1, psi):                                 # psi 시점 이전까지는 관리상태 데이터\n","            x[i] = ar1 * x[i - 1] + e[i]                        # 각 시점에서의 값은 이전 시점의 값에 자기상관 계수 ar1을 곱한 것과 현재 시점의 노이즈를 더한 값으로 설정\n","\n","            # psi 시점 이후의 데이터 생성 및 변동성 적용\n","            if i >= psi:                                        # psi 시점 이후에는 각 에러 항에 gamma 값을 곱하여 에러 항의 변동성을 조절\n","                e[i] = gamma * e[i]\n","                x[i] = ar1 * x[i - 1] + e[i] + delta * sigma    # delta(변동성 크기 조절하는 매개변수)를 통한 추가 변동성 적용\n","\n","        # 최종 데이터 반환 (각 run에 대해 생성된 시계열 데이터를 저장)\n","        y[j] = x\n","\n","    return y\n","\n","\n","# 다양한 매개변수 조합에 대한 시계열 데이터 세트 생성\n","def totaldat(run,length):\n","    # 빈 데이터 배열 초기화\n","    y = np.zeros(shape=(len(phi1), run, length))\n","    # 매개변수 조합별 데이터 생성\n","    for i, phi in enumerate(phi1):\n","        y[i]= ar(phi, de1, ga, psi1, length, run)\n","\n","    return y.reshape(run * len(phi1), length)                   # 생성된 데이터를 적절한 형태로 재배열\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uzu_D7RRGuP1"},"outputs":[],"source":["# 훈련용 시계열 데이터\n","# 데이터 생성 및 변형\n","train_x = totaldat(trainrun,length)                             # 훈련용 시계열 데이터 생성\n","train_x = train_x.reshape(trainrun*len(phi1),length)\n","\n","# 검증용 시계열 데이터\n","# 데이터 생성 및 변형\n","valid_x = totaldat(run = validrun, length = length)\n","valid_x = valid_x.reshape(validrun*len(phi1),length)\n","\n","# PyTorch 텐서로 변환 및 장치 할당\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train_x = torch.FloatTensor(train_x).to(device)\n","valid_x = torch.FloatTensor(valid_x).to(device)\n","\n","# DataLoader 설정\n","trainloader = DataLoader(train_x, shuffle=True)                  # 데이터셋에서 미니배치 자동으로 생성 후 모델 학습 및 평가 시 배치 처리를 용이하게 함 (훈련에서는 데이터를 섞어 학습 과정에서의 일반화 능력 향상)\n","validloader = DataLoader(valid_x, shuffle=False)                # 학습 및 검증에서는 데이터 순서 유지"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVNRYLmmGuP1","outputId":"1e32c3ea-2a9d-4036-8eff-e23078808dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 12])\n"]}],"source":["for data in trainloader:\n","    print(data.shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmkuDezXGuP2","outputId":"02db9be9-5f08-4614-d461-add4432f74f5"},"outputs":[{"data":{"text/plain":["1000"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(trainloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0T06ErMGuP2","outputId":"794b9fc8-9e54-41ac-84d0-b3f7716df4ad"},"outputs":[{"data":{"text/plain":["500"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(validloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bI8H07QGGuP2","outputId":"cd6b22ee-bb85-48a3-f30e-201991203133"},"outputs":[{"data":{"text/plain":["12"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_x.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"asdonTrQGuP2"},"source":["#### LSTM Autoencoder 구조"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyOuoUH_GuP3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# LSTM Encoder 클래스 정의\n","class LSTMEncoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, latent_dim):\n","        super(LSTMEncoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.latent_dim = latent_dim\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True)\n","        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n","\n","    def forward(self, x):\n","        _, (hidden, _) = self.lstm(x)\n","        return self.hidden_to_latent(hidden[-1])\n","\n","# LSTM Decoder 클래스 정의\n","class LSTMDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, output_dim, seq_length):\n","        super(LSTMDecoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.seq_length = seq_length\n","        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n","        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=2, batch_first=True)\n","        self.outputs_to_data = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, z):\n","        z = self.latent_to_hidden(z).unsqueeze(0)\n","        repeated_z = z.repeat(self.seq_length, 1, 1).transpose(0, 1)\n","        lstm_out, _ = self.lstm(repeated_z)\n","        return self.outputs_to_data(lstm_out)\n","\n","# 전체 LSTM 오토인코더 모델\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self, input_dim, seq_length, hidden_dim=128, latent_dim=1):\n","        super(LSTMAutoencoder, self).__init__()\n","        self.encoder = LSTMEncoder(input_dim, hidden_dim, latent_dim)\n","        self.decoder = LSTMDecoder(latent_dim, hidden_dim, input_dim, seq_length)\n","\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        return self.decoder(z)\n","\n","# 모델 인스턴스 생성 및 설정\n","model = LSTMAutoencoder(input_dim=length, seq_length=length, hidden_dim=128, latent_dim=1)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fPxL50AGuP3"},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-GkkoWgGuP3","outputId":"ca5d9c67-5f29-45e3-cc8a-b87b23e0b778"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Pro\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 12])) that is different to the input size (torch.Size([1, 12, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.0934, Validation Loss: 0.0916\n","Validation loss decreased (inf --> 0.091558).  Saving model ...\n","Epoch 2, Train Loss: 0.0921, Validation Loss: 0.0906\n","Validation loss decreased (0.091558 --> 0.090612).  Saving model ...\n","Epoch 3, Train Loss: 0.0913, Validation Loss: 0.0901\n","Validation loss decreased (0.090612 --> 0.090099).  Saving model ...\n","Epoch 4, Train Loss: 0.0910, Validation Loss: 0.0898\n","Validation loss decreased (0.090099 --> 0.089843).  Saving model ...\n","Epoch 5, Train Loss: 0.0908, Validation Loss: 0.0897\n","Validation loss decreased (0.089843 --> 0.089722).  Saving model ...\n","Epoch 6, Train Loss: 0.0907, Validation Loss: 0.0897\n","Validation loss decreased (0.089722 --> 0.089658).  Saving model ...\n","Epoch 7, Train Loss: 0.0906, Validation Loss: 0.0896\n","Validation loss decreased (0.089658 --> 0.089609).  Saving model ...\n","Epoch 8, Train Loss: 0.0906, Validation Loss: 0.0896\n","Validation loss decreased (0.089609 --> 0.089561).  Saving model ...\n","Epoch 9, Train Loss: 0.0905, Validation Loss: 0.0895\n","Validation loss decreased (0.089561 --> 0.089507).  Saving model ...\n","Epoch 10, Train Loss: 0.0904, Validation Loss: 0.0894\n","Validation loss decreased (0.089507 --> 0.089442).  Saving model ...\n","Epoch 11, Train Loss: 0.0904, Validation Loss: 0.0894\n","Validation loss decreased (0.089442 --> 0.089366).  Saving model ...\n","Epoch 12, Train Loss: 0.0903, Validation Loss: 0.0893\n","Validation loss decreased (0.089366 --> 0.089274).  Saving model ...\n","Epoch 13, Train Loss: 0.0902, Validation Loss: 0.0892\n","Validation loss decreased (0.089274 --> 0.089166).  Saving model ...\n","Epoch 14, Train Loss: 0.0900, Validation Loss: 0.0890\n","Validation loss decreased (0.089166 --> 0.089036).  Saving model ...\n","Epoch 15, Train Loss: 0.0899, Validation Loss: 0.0889\n","Validation loss decreased (0.089036 --> 0.088880).  Saving model ...\n","Epoch 16, Train Loss: 0.0897, Validation Loss: 0.0887\n","Validation loss decreased (0.088880 --> 0.088691).  Saving model ...\n","Epoch 17, Train Loss: 0.0895, Validation Loss: 0.0885\n","Validation loss decreased (0.088691 --> 0.088455).  Saving model ...\n","Epoch 18, Train Loss: 0.0893, Validation Loss: 0.0882\n","Validation loss decreased (0.088455 --> 0.088178).  Saving model ...\n","Epoch 19, Train Loss: 0.0889, Validation Loss: 0.0878\n","Validation loss decreased (0.088178 --> 0.087833).  Saving model ...\n","Epoch 20, Train Loss: 0.0885, Validation Loss: 0.0874\n","Validation loss decreased (0.087833 --> 0.087400).  Saving model ...\n","Epoch 21, Train Loss: 0.0881, Validation Loss: 0.0869\n","Validation loss decreased (0.087400 --> 0.086858).  Saving model ...\n","Epoch 22, Train Loss: 0.0874, Validation Loss: 0.0862\n","Validation loss decreased (0.086858 --> 0.086179).  Saving model ...\n","Epoch 23, Train Loss: 0.0867, Validation Loss: 0.0853\n","Validation loss decreased (0.086179 --> 0.085299).  Saving model ...\n","Epoch 24, Train Loss: 0.0857, Validation Loss: 0.0842\n","Validation loss decreased (0.085299 --> 0.084178).  Saving model ...\n","Epoch 25, Train Loss: 0.0844, Validation Loss: 0.0827\n","Validation loss decreased (0.084178 --> 0.082745).  Saving model ...\n","Epoch 26, Train Loss: 0.0827, Validation Loss: 0.0809\n","Validation loss decreased (0.082745 --> 0.080858).  Saving model ...\n","Epoch 27, Train Loss: 0.0806, Validation Loss: 0.0784\n","Validation loss decreased (0.080858 --> 0.078447).  Saving model ...\n","Epoch 28, Train Loss: 0.0778, Validation Loss: 0.0753\n","Validation loss decreased (0.078447 --> 0.075270).  Saving model ...\n","Epoch 29, Train Loss: 0.0741, Validation Loss: 0.0710\n","Validation loss decreased (0.075270 --> 0.071036).  Saving model ...\n","Epoch 30, Train Loss: 0.0693, Validation Loss: 0.0657\n","Validation loss decreased (0.071036 --> 0.065673).  Saving model ...\n","Epoch 31, Train Loss: 0.0634, Validation Loss: 0.0592\n","Validation loss decreased (0.065673 --> 0.059235).  Saving model ...\n","Epoch 32, Train Loss: 0.0565, Validation Loss: 0.0520\n","Validation loss decreased (0.059235 --> 0.051976).  Saving model ...\n","Epoch 33, Train Loss: 0.0491, Validation Loss: 0.0447\n","Validation loss decreased (0.051976 --> 0.044677).  Saving model ...\n","Epoch 34, Train Loss: 0.0424, Validation Loss: 0.0388\n","Validation loss decreased (0.044677 --> 0.038842).  Saving model ...\n","Epoch 35, Train Loss: 0.0375, Validation Loss: 0.0350\n","Validation loss decreased (0.038842 --> 0.034975).  Saving model ...\n","Epoch 36, Train Loss: 0.0347, Validation Loss: 0.0330\n","Validation loss decreased (0.034975 --> 0.033026).  Saving model ...\n","Epoch 37, Train Loss: 0.0333, Validation Loss: 0.0320\n","Validation loss decreased (0.033026 --> 0.032025).  Saving model ...\n","Epoch 38, Train Loss: 0.0324, Validation Loss: 0.0312\n","Validation loss decreased (0.032025 --> 0.031151).  Saving model ...\n","Epoch 39, Train Loss: 0.0316, Validation Loss: 0.0303\n","Validation loss decreased (0.031151 --> 0.030278).  Saving model ...\n","Epoch 40, Train Loss: 0.0306, Validation Loss: 0.0293\n","Validation loss decreased (0.030278 --> 0.029322).  Saving model ...\n","Epoch 41, Train Loss: 0.0295, Validation Loss: 0.0281\n","Validation loss decreased (0.029322 --> 0.028113).  Saving model ...\n","Epoch 42, Train Loss: 0.0282, Validation Loss: 0.0268\n","Validation loss decreased (0.028113 --> 0.026846).  Saving model ...\n","Epoch 43, Train Loss: 0.0269, Validation Loss: 0.0255\n","Validation loss decreased (0.026846 --> 0.025476).  Saving model ...\n","Epoch 44, Train Loss: 0.0255, Validation Loss: 0.0241\n","Validation loss decreased (0.025476 --> 0.024068).  Saving model ...\n","Epoch 45, Train Loss: 0.0240, Validation Loss: 0.0227\n","Validation loss decreased (0.024068 --> 0.022662).  Saving model ...\n","Epoch 46, Train Loss: 0.0226, Validation Loss: 0.0213\n","Validation loss decreased (0.022662 --> 0.021318).  Saving model ...\n","Epoch 47, Train Loss: 0.0212, Validation Loss: 0.0200\n","Validation loss decreased (0.021318 --> 0.019979).  Saving model ...\n","Epoch 48, Train Loss: 0.0199, Validation Loss: 0.0187\n","Validation loss decreased (0.019979 --> 0.018703).  Saving model ...\n","Epoch 49, Train Loss: 0.0186, Validation Loss: 0.0175\n","Validation loss decreased (0.018703 --> 0.017502).  Saving model ...\n","Epoch 50, Train Loss: 0.0174, Validation Loss: 0.0164\n","Validation loss decreased (0.017502 --> 0.016374).  Saving model ...\n","Epoch 51, Train Loss: 0.0163, Validation Loss: 0.0153\n","Validation loss decreased (0.016374 --> 0.015329).  Saving model ...\n","Epoch 52, Train Loss: 0.0153, Validation Loss: 0.0144\n","Validation loss decreased (0.015329 --> 0.014397).  Saving model ...\n","Epoch 53, Train Loss: 0.0144, Validation Loss: 0.0136\n","Validation loss decreased (0.014397 --> 0.013573).  Saving model ...\n","Epoch 54, Train Loss: 0.0135, Validation Loss: 0.0128\n","Validation loss decreased (0.013573 --> 0.012809).  Saving model ...\n","Epoch 55, Train Loss: 0.0128, Validation Loss: 0.0121\n","Validation loss decreased (0.012809 --> 0.012113).  Saving model ...\n","Epoch 56, Train Loss: 0.0121, Validation Loss: 0.0115\n","Validation loss decreased (0.012113 --> 0.011465).  Saving model ...\n","Epoch 57, Train Loss: 0.0114, Validation Loss: 0.0108\n","Validation loss decreased (0.011465 --> 0.010849).  Saving model ...\n","Epoch 58, Train Loss: 0.0108, Validation Loss: 0.0103\n","Validation loss decreased (0.010849 --> 0.010284).  Saving model ...\n","Epoch 59, Train Loss: 0.0103, Validation Loss: 0.0097\n","Validation loss decreased (0.010284 --> 0.009731).  Saving model ...\n","Epoch 60, Train Loss: 0.0097, Validation Loss: 0.0092\n","Validation loss decreased (0.009731 --> 0.009214).  Saving model ...\n","Epoch 61, Train Loss: 0.0092, Validation Loss: 0.0087\n","Validation loss decreased (0.009214 --> 0.008705).  Saving model ...\n","Epoch 62, Train Loss: 0.0087, Validation Loss: 0.0082\n","Validation loss decreased (0.008705 --> 0.008239).  Saving model ...\n","Epoch 63, Train Loss: 0.0082, Validation Loss: 0.0078\n","Validation loss decreased (0.008239 --> 0.007781).  Saving model ...\n","Epoch 64, Train Loss: 0.0077, Validation Loss: 0.0073\n","Validation loss decreased (0.007781 --> 0.007334).  Saving model ...\n","Epoch 65, Train Loss: 0.0073, Validation Loss: 0.0069\n","Validation loss decreased (0.007334 --> 0.006914).  Saving model ...\n","Epoch 66, Train Loss: 0.0069, Validation Loss: 0.0065\n","Validation loss decreased (0.006914 --> 0.006518).  Saving model ...\n","Epoch 67, Train Loss: 0.0065, Validation Loss: 0.0061\n","Validation loss decreased (0.006518 --> 0.006121).  Saving model ...\n","Epoch 68, Train Loss: 0.0061, Validation Loss: 0.0057\n","Validation loss decreased (0.006121 --> 0.005745).  Saving model ...\n","Epoch 69, Train Loss: 0.0057, Validation Loss: 0.0054\n","Validation loss decreased (0.005745 --> 0.005386).  Saving model ...\n","Epoch 70, Train Loss: 0.0054, Validation Loss: 0.0050\n","Validation loss decreased (0.005386 --> 0.005040).  Saving model ...\n","Epoch 71, Train Loss: 0.0050, Validation Loss: 0.0047\n","Validation loss decreased (0.005040 --> 0.004704).  Saving model ...\n","Epoch 72, Train Loss: 0.0047, Validation Loss: 0.0044\n","Validation loss decreased (0.004704 --> 0.004388).  Saving model ...\n","Epoch 73, Train Loss: 0.0044, Validation Loss: 0.0041\n","Validation loss decreased (0.004388 --> 0.004094).  Saving model ...\n","Epoch 74, Train Loss: 0.0041, Validation Loss: 0.0038\n","Validation loss decreased (0.004094 --> 0.003811).  Saving model ...\n","Epoch 75, Train Loss: 0.0038, Validation Loss: 0.0035\n","Validation loss decreased (0.003811 --> 0.003547).  Saving model ...\n","Epoch 76, Train Loss: 0.0035, Validation Loss: 0.0033\n","Validation loss decreased (0.003547 --> 0.003297).  Saving model ...\n","Epoch 77, Train Loss: 0.0033, Validation Loss: 0.0031\n","Validation loss decreased (0.003297 --> 0.003055).  Saving model ...\n","Epoch 78, Train Loss: 0.0031, Validation Loss: 0.0028\n","Validation loss decreased (0.003055 --> 0.002830).  Saving model ...\n","Epoch 79, Train Loss: 0.0028, Validation Loss: 0.0026\n","Validation loss decreased (0.002830 --> 0.002618).  Saving model ...\n","Epoch 80, Train Loss: 0.0026, Validation Loss: 0.0024\n","Validation loss decreased (0.002618 --> 0.002418).  Saving model ...\n","Epoch 81, Train Loss: 0.0024, Validation Loss: 0.0022\n","Validation loss decreased (0.002418 --> 0.002228).  Saving model ...\n","Epoch 82, Train Loss: 0.0022, Validation Loss: 0.0021\n","Validation loss decreased (0.002228 --> 0.002053).  Saving model ...\n","Epoch 83, Train Loss: 0.0021, Validation Loss: 0.0019\n","Validation loss decreased (0.002053 --> 0.001882).  Saving model ...\n","Epoch 84, Train Loss: 0.0019, Validation Loss: 0.0017\n","Validation loss decreased (0.001882 --> 0.001728).  Saving model ...\n","Epoch 85, Train Loss: 0.0018, Validation Loss: 0.0016\n","Validation loss decreased (0.001728 --> 0.001581).  Saving model ...\n","Epoch 86, Train Loss: 0.0016, Validation Loss: 0.0014\n","Validation loss decreased (0.001581 --> 0.001439).  Saving model ...\n","Epoch 87, Train Loss: 0.0015, Validation Loss: 0.0013\n","Validation loss decreased (0.001439 --> 0.001308).  Saving model ...\n","Epoch 88, Train Loss: 0.0013, Validation Loss: 0.0012\n","Validation loss decreased (0.001308 --> 0.001186).  Saving model ...\n","Epoch 89, Train Loss: 0.0012, Validation Loss: 0.0011\n","Validation loss decreased (0.001186 --> 0.001075).  Saving model ...\n","Epoch 90, Train Loss: 0.0011, Validation Loss: 0.0010\n","Validation loss decreased (0.001075 --> 0.000970).  Saving model ...\n","Epoch 91, Train Loss: 0.0010, Validation Loss: 0.0009\n","Validation loss decreased (0.000970 --> 0.000877).  Saving model ...\n","Epoch 92, Train Loss: 0.0009, Validation Loss: 0.0008\n","Validation loss decreased (0.000877 --> 0.000790).  Saving model ...\n","Epoch 93, Train Loss: 0.0008, Validation Loss: 0.0007\n","Validation loss decreased (0.000790 --> 0.000709).  Saving model ...\n","Epoch 94, Train Loss: 0.0007, Validation Loss: 0.0006\n","Validation loss decreased (0.000709 --> 0.000637).  Saving model ...\n","Epoch 95, Train Loss: 0.0007, Validation Loss: 0.0006\n","Validation loss decreased (0.000637 --> 0.000574).  Saving model ...\n","Epoch 96, Train Loss: 0.0006, Validation Loss: 0.0005\n","Validation loss decreased (0.000574 --> 0.000514).  Saving model ...\n","Epoch 97, Train Loss: 0.0006, Validation Loss: 0.0005\n","Validation loss decreased (0.000514 --> 0.000464).  Saving model ...\n","Epoch 98, Train Loss: 0.0005, Validation Loss: 0.0004\n","Validation loss decreased (0.000464 --> 0.000417).  Saving model ...\n","Epoch 99, Train Loss: 0.0005, Validation Loss: 0.0004\n","Validation loss decreased (0.000417 --> 0.000377).  Saving model ...\n","Epoch 100, Train Loss: 0.0004, Validation Loss: 0.0003\n","Validation loss decreased (0.000377 --> 0.000340).  Saving model ...\n","Epoch 101, Train Loss: 0.0004, Validation Loss: 0.0003\n","Validation loss decreased (0.000340 --> 0.000308).  Saving model ...\n","Epoch 102, Train Loss: 0.0004, Validation Loss: 0.0003\n","Validation loss decreased (0.000308 --> 0.000280).  Saving model ...\n","Epoch 103, Train Loss: 0.0003, Validation Loss: 0.0003\n","Validation loss decreased (0.000280 --> 0.000255).  Saving model ...\n","Epoch 104, Train Loss: 0.0003, Validation Loss: 0.0002\n","Validation loss decreased (0.000255 --> 0.000234).  Saving model ...\n","Epoch 105, Train Loss: 0.0003, Validation Loss: 0.0002\n","Validation loss decreased (0.000234 --> 0.000214).  Saving model ...\n","Epoch 106, Train Loss: 0.0003, Validation Loss: 0.0002\n","Validation loss decreased (0.000214 --> 0.000198).  Saving model ...\n","Epoch 107, Train Loss: 0.0002, Validation Loss: 0.0002\n","Validation loss decreased (0.000198 --> 0.000183).  Saving model ...\n","Epoch 108, Train Loss: 0.0002, Validation Loss: 0.0002\n","Validation loss decreased (0.000183 --> 0.000170).  Saving model ...\n","Epoch 109, Train Loss: 0.0002, Validation Loss: 0.0002\n","Validation loss decreased (0.000170 --> 0.000158).  Saving model ...\n","Epoch 110, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000158 --> 0.000148).  Saving model ...\n","Epoch 111, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000148 --> 0.000139).  Saving model ...\n","Epoch 112, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000139 --> 0.000131).  Saving model ...\n","Epoch 113, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000131 --> 0.000124).  Saving model ...\n","Epoch 114, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000124 --> 0.000117).  Saving model ...\n","Epoch 115, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000117 --> 0.000111).  Saving model ...\n","Epoch 116, Train Loss: 0.0002, Validation Loss: 0.0001\n","Validation loss decreased (0.000111 --> 0.000106).  Saving model ...\n","Epoch 117, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000106 --> 0.000101).  Saving model ...\n","Epoch 118, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000101 --> 0.000096).  Saving model ...\n","Epoch 119, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000096 --> 0.000092).  Saving model ...\n","Epoch 120, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000092 --> 0.000087).  Saving model ...\n","Epoch 121, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000087 --> 0.000084).  Saving model ...\n","Epoch 122, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000084 --> 0.000080).  Saving model ...\n","Epoch 123, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000080 --> 0.000077).  Saving model ...\n","Epoch 124, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000077 --> 0.000073).  Saving model ...\n","Epoch 125, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000073 --> 0.000070).  Saving model ...\n","Epoch 126, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000070 --> 0.000067).  Saving model ...\n","Epoch 127, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000067 --> 0.000064).  Saving model ...\n","Epoch 128, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000064 --> 0.000061).  Saving model ...\n","Epoch 129, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000061 --> 0.000059).  Saving model ...\n","Epoch 130, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000059 --> 0.000056).  Saving model ...\n","Epoch 131, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000056 --> 0.000054).  Saving model ...\n","Epoch 132, Train Loss: 0.0001, Validation Loss: 0.0001\n","Validation loss decreased (0.000054 --> 0.000051).  Saving model ...\n","Epoch 133, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000051 --> 0.000049).  Saving model ...\n","Epoch 134, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000049 --> 0.000047).  Saving model ...\n","Epoch 135, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000047 --> 0.000045).  Saving model ...\n","Epoch 136, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000045 --> 0.000043).  Saving model ...\n","Epoch 137, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000043 --> 0.000041).  Saving model ...\n","Epoch 138, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000041 --> 0.000039).  Saving model ...\n","Epoch 139, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000039 --> 0.000038).  Saving model ...\n","Epoch 140, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000038 --> 0.000036).  Saving model ...\n","Epoch 141, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000036 --> 0.000034).  Saving model ...\n","Epoch 142, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000034 --> 0.000032).  Saving model ...\n","Epoch 143, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000032 --> 0.000031).  Saving model ...\n","Epoch 144, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000031 --> 0.000030).  Saving model ...\n","Epoch 145, Train Loss: 0.0001, Validation Loss: 0.0000\n","Validation loss decreased (0.000030 --> 0.000028).  Saving model ...\n","Epoch 146, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000028 --> 0.000027).  Saving model ...\n","Epoch 147, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000027 --> 0.000026).  Saving model ...\n","Epoch 148, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000026 --> 0.000024).  Saving model ...\n","Epoch 149, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000024 --> 0.000023).  Saving model ...\n","Epoch 150, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000023 --> 0.000022).  Saving model ...\n","Epoch 151, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000022 --> 0.000022).  Saving model ...\n","Epoch 152, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000022 --> 0.000020).  Saving model ...\n","Epoch 153, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000020 --> 0.000019).  Saving model ...\n","Epoch 154, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000019 --> 0.000019).  Saving model ...\n","Epoch 155, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000019 --> 0.000018).  Saving model ...\n","Epoch 156, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000018 --> 0.000017).  Saving model ...\n","Epoch 157, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000017 --> 0.000016).  Saving model ...\n","Epoch 158, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000016 --> 0.000016).  Saving model ...\n","Epoch 159, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000016 --> 0.000015).  Saving model ...\n","Epoch 160, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000015 --> 0.000014).  Saving model ...\n","Epoch 161, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000014 --> 0.000014).  Saving model ...\n","Epoch 162, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000014 --> 0.000013).  Saving model ...\n","Epoch 163, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000013 --> 0.000013).  Saving model ...\n","Epoch 164, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000013 --> 0.000012).  Saving model ...\n","Epoch 165, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000012 --> 0.000011).  Saving model ...\n","Epoch 166, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000011 --> 0.000011).  Saving model ...\n","Epoch 167, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000011 --> 0.000011).  Saving model ...\n","Epoch 168, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000011 --> 0.000010).  Saving model ...\n","Epoch 169, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000010 --> 0.000010).  Saving model ...\n","Epoch 170, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000010 --> 0.000009).  Saving model ...\n","Epoch 171, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000009 --> 0.000009).  Saving model ...\n","Epoch 172, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000009 --> 0.000009).  Saving model ...\n","Epoch 173, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000009 --> 0.000009).  Saving model ...\n","Epoch 174, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000009 --> 0.000008).  Saving model ...\n","Epoch 175, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000008 --> 0.000008).  Saving model ...\n","Epoch 176, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000008 --> 0.000008).  Saving model ...\n","Epoch 177, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000008 --> 0.000008).  Saving model ...\n","Epoch 178, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000008 --> 0.000007).  Saving model ...\n","Epoch 179, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 180, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000007).  Saving model ...\n","Epoch 181, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000007).  Saving model ...\n","Epoch 182, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000007).  Saving model ...\n","Epoch 183, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000007).  Saving model ...\n","Epoch 184, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000007).  Saving model ...\n","Epoch 185, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000007 --> 0.000006).  Saving model ...\n","Epoch 186, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 187, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 188, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 189, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 190, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 191, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 192, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000006).  Saving model ...\n","Epoch 193, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000006 --> 0.000005).  Saving model ...\n","Epoch 194, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 195, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 196, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 197, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 198, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 199, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 200, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 201, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 202, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 203, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000005).  Saving model ...\n","Epoch 204, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000005 --> 0.000004).  Saving model ...\n","Epoch 205, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 206, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 207, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 208, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 209, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 210, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 211, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 212, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 213, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 214, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 215, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 216, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 217, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 218, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000004).  Saving model ...\n","Epoch 219, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000004 --> 0.000003).  Saving model ...\n","Epoch 220, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 221, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 222, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 223, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 224, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 225, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 226, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 227, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 228, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 229, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 230, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 231, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 232, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 233, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 234, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 235, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 236, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 237, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 238, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 239, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000003).  Saving model ...\n","Epoch 240, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000003 --> 0.000002).  Saving model ...\n","Epoch 241, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 242, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 243, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 244, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 245, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 246, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 247, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 248, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 249, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 250, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 251, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 252, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 253, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 254, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 255, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 256, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 257, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 258, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 259, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 260, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 261, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 262, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 263, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 264, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 265, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 266, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 267, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 268, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 269, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 270, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 271, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 272, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 273, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000002).  Saving model ...\n","Epoch 274, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 275, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000002 --> 0.000001).  Saving model ...\n","Epoch 276, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 277, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 278, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 279, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 280, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 281, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 282, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 283, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 284, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 285, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 286, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 287, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 288, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 289, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 290, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 291, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 292, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 293, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 294, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 295, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 296, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 297, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 298, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 299, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 300, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 301, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 3 out of 5\n","Epoch 302, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 303, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 304, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 305, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 306, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 307, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 308, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 309, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 310, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 311, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 312, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 313, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 314, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 315, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 316, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 317, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 318, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 319, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 320, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 321, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 322, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 323, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 324, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 325, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 326, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 327, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 328, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 329, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 330, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 331, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 3 out of 5\n","Epoch 332, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 333, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 334, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 335, Train Loss: 0.0000, Validation Loss: 0.0000\n","Validation loss decreased (0.000001 --> 0.000001).  Saving model ...\n","Epoch 336, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 1 out of 5\n","Epoch 337, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 2 out of 5\n","Epoch 338, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 3 out of 5\n","Epoch 339, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 4 out of 5\n","Epoch 340, Train Loss: 0.0000, Validation Loss: 0.0000\n","EarlyStopping counter: 5 out of 5\n","Early stopping\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x500 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["def training(epochs):\n","    loss_ = []\n","    valoss_ = []\n","\n","    # epoch 별 훈련 및 검증 루프 (trainloader를 반복하여 모델 훈련시키고, validloader를 사용해 모델의 성능 검증)\n","    for epoch in range(epochs):\n","        running_train_loss = 0.0\n","        running_vall_loss = 0.0\n","\n","        # 훈련 과정\n","        for data in trainloader:\n","            model.train()\n","            inputs = data\n","            inputs = inputs.to(device)\n","            optimizer.zero_grad()                                                   # Optimizer Gradient를 0으로 초기화. 이는 각 미니배치마다 Gradient가 누적되는 것을 방지\n","            predicted_outputs = model(inputs)                                       # 모델을 사용하여 입력 데이터에 대한 예측값을 계산\n","            train_loss = criterion(predicted_outputs, inputs)                       # 계산된 예측값과 실제 값 간의 손실을 계산\n","            train_loss.backward()                                                   # 손실에 대한 역전파를 수행하여 Gradient를 계산\n","            optimizer.step()\n","            running_train_loss += train_loss.item()\n","\n","        # Calculate average training loss for the epoch\n","        avg_train_loss = running_train_loss / len(trainloader)\n","        loss_.append(avg_train_loss)\n","\n","        # 검증 과정\n","        with torch.no_grad():\n","            model.eval()\n","            for data in validloader:\n","                inputs = data\n","                inputs = inputs.to(device)\n","                predicted_outputs = model(inputs)\n","                val_loss = criterion(predicted_outputs, inputs)\n","                running_vall_loss += val_loss.item()\n","\n","            avg_val_loss = running_vall_loss / len(validloader)\n","            valoss_.append(avg_val_loss)\n","\n","        print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n","\n","        # Early Stopping\n","        early_stopping(avg_val_loss, model)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","\n","    # Save model\n","    saveModel()\n","    return loss_, valoss_\n","\n","loss_, valoss_ = training(epochs=400)\n","\n","# Loss visualization\n","plt.figure(figsize=(10, 5))\n","plt.plot(loss_, label='Train Loss')\n","plt.plot(valoss_, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D0fRiM8cGuP3"},"source":["#### ARL0 구하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TepwrrK6GuP4"},"outputs":[],"source":["np.random.seed(1)\n","\n","l = 12\n","\n","# 관리상태 / 이상상태 데이터 생성 함수\n","def argen(ar, psi, delta,gamma, length) :\n","\n","    e = np.random.normal(loc=0, scale = 1,size = length)\n","    sigma = math.sqrt(1 / (1 - pow(ar, 2)))\n","    x = np.array(np.repeat(0, length), dtype= np.float64)\n","    x[0] = e[0]\n","    z = np.array(np.repeat(0, length), dtype=np.float64)\n","\n","    for i in range(1, psi):\n","        x[i] = ar * x[i-1] + e[i]\n","        z[i] = x[i]\n","    for i in range(psi,len(x)):\n","        x[i] = ar * x[i - 1] + gamma*e[i]\n","        z[i] = x[i]\n","    for i in range(psi,len(z)):\n","        z[i] = z[i] + delta * sigma\n","\n","    return z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-U6X278hGuP4"},"outputs":[],"source":["def arl(ar,delta,gamma, run, length,cl) :\n","    rl = np.array([], dtype=np.float64)\n","\n","    for i in tqdm(range(run)) :\n","        y = argen(ar=ar, psi=l-1, delta=delta, gamma = gamma,length=length)\n","        a = np.array([length-l])\n","        x = np.zeros(shape=(length-l, l))\n","        for j in range(length-l):\n","            x[j] = y[j: j + l]\n","        x = torch.FloatTensor(x).to(device)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for j in range(0,len(x)):\n","                input = x[[j]]\n","\n","                output = model(input)\n","\n","                mse_loss = nn.MSELoss()\n","                loss = mse_loss(output[0], input[0]) # output shape 확인\n","\n","                if (loss > cl).any() :\n","                    a = np.array([j + 1])\n","                    break\n","                elif j == len(x) - 1:\n","                    a = len(x)\n","\n","            rl = np.append(rl,a)\n","\n","    arl = np.mean(rl)\n","    return arl"]},{"cell_type":"markdown","metadata":{"id":"8kVWIbQdGuP4"},"source":["#### ARL1 구하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nOlnJ7AGuP4"},"outputs":[],"source":["def arl1(ar,run,length,cl):\n","    a5 = arl(ar, 0.5, 1, run, length, cl)\n","    a1 = arl(ar, 1, 1, run, length, cl)\n","    a2 = arl(ar, 2, 1, run, length, cl)\n","    a3 = arl(ar, 3, 1, run, length, cl)\n","    b5 = arl(ar, 0.5, 1.5,run, length, cl)\n","    b1 = arl(ar, 1, 1.5, run, length, cl)\n","    b2 = arl(ar, 2, 1.5, run, length, cl)\n","    b3 = arl(ar, 3, 1, run, length, cl)\n","    c1 = arl(ar, 0, 1.5, run, length, cl)\n","    c2 = arl(ar, 0, 2, run, length, cl)\n","    c3 = arl(ar, 0, 3, run, length, cl)\n","    print(f'0.5: {a5}, 1:{a1},2:{a2},3:{a3}')\n","    print(f'0.5:{b5},1:{b1},2:{b2},3:{b3}')\n","    print(f'1.5:{c1},2:{c2},3:{c3}')"]},{"cell_type":"markdown","metadata":{"id":"kUzpO-EsGuP4"},"source":["#### phi = 0.25 일 때"]},{"cell_type":"markdown","metadata":{"id":"j-32PWJfGuP4"},"source":["##### ARL0 (threshold 임의추정)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIcyPdNwGuP5","outputId":"a2ed5955-c106-48c6-b9e3-58657804e1aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [25:50<00:00,  6.45it/s]\n"]},{"data":{"text/plain":["278.8572"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 2.18)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGL3gaVLGuP5","outputId":"d906cec9-7613-4626-eb5f-e59841bb3f42"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [33:54<00:00,  4.92it/s]\n"]},{"data":{"text/plain":["370.6138"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["arl(ar = 0.25, delta = 0, gamma = 1, run = 10000, length = 1000, cl = 2.3)"]},{"cell_type":"markdown","metadata":{"id":"AOz6xMMsGuP6"},"source":["##### ARL1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geg40O4RGuP6","outputId":"788ca2a9-411e-45c6-82f0-ae581b9f01b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 10000/10000 [09:45<00:00, 17.08it/s]\n","100%|██████████| 10000/10000 [02:09<00:00, 77.16it/s]\n","100%|██████████| 10000/10000 [00:37<00:00, 270.27it/s]\n","100%|██████████| 10000/10000 [00:22<00:00, 437.49it/s]\n","100%|██████████| 10000/10000 [01:29<00:00, 111.70it/s]\n","100%|██████████| 10000/10000 [01:01<00:00, 161.84it/s]\n","100%|██████████| 10000/10000 [00:34<00:00, 292.78it/s]\n","100%|██████████| 10000/10000 [00:22<00:00, 436.55it/s]\n","100%|██████████| 10000/10000 [01:41<00:00, 98.36it/s]\n","100%|██████████| 10000/10000 [00:49<00:00, 203.06it/s]\n","100%|██████████| 10000/10000 [00:29<00:00, 333.70it/s]"]},{"name":"stdout","output_type":"stream","text":["0.5: 103.3649, 1:21.8216,2:4.9169,3:2.4772\n","0.5:14.2495,1:9.4149,2:4.4184,3:2.4789\n","1.5:16.9843,2:7.1939,3:3.586\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["arl1(ar = 0.25, run = 10000, length = 1000, cl = 2.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WNIaWP4GuP6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"my_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}